{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c993fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "import scipy.io as sio\n",
    "from pathlib import Path\n",
    "# from torch.nn.utils import weight_norm\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2b68ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.3\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ef6bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando PyTorch 2.9.0+cu128 | GPU disponible: True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando PyTorch {torch.__version__} | GPU disponible: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d922cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Opcional) ligeras optimizaciones en GPU\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b1ce95",
   "metadata": {},
   "source": [
    "# Rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa38cd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path().resolve().parents[0]\n",
    "DATA_PROC_DIR = BASE_DIR / \"data\" / \"processed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4443c9fa",
   "metadata": {},
   "source": [
    "# Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e45b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = DATA_PROC_DIR / \"weather_data_processed.mat\"\n",
    "WS_data = sio.loadmat(path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a84e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Malla PINN\n",
    "X_PINN = WS_data[\"X_PINN\"]\n",
    "Y_PINN = WS_data[\"Y_PINN\"]\n",
    "T_PINN = WS_data[\"T_PINN\"]\n",
    "# Data WS\n",
    "T_WS = WS_data[\"T_WS\"]\n",
    "P_WS = WS_data[\"P_WS\"]\n",
    "U_WS = WS_data[\"U_WS\"]\n",
    "V_WS = WS_data[\"V_WS\"]\n",
    "X_WS = WS_data[\"X_WS\"]\n",
    "Y_WS = WS_data[\"Y_WS\"]\n",
    "# Data val\n",
    "WS_val = WS_data[\"WS_val\"]\n",
    "T_val = WS_data[\"T_val\"]\n",
    "P_val = WS_data[\"P_val\"]\n",
    "U_val = WS_data[\"U_val\"]\n",
    "V_val = WS_data[\"V_val\"]\n",
    "X_val = WS_data[\"X_val\"]\n",
    "Y_val = WS_data[\"Y_val\"]\n",
    "Z_val = WS_data[\"Z_val\"]\n",
    "\n",
    "L = WS_data[\"L\"]\n",
    "W = WS_data[\"W\"]\n",
    "P0 = WS_data[\"P0\"]\n",
    "Re = WS_data[\"Re\"]\n",
    "\n",
    "batch_PINN = WS_data[\"batch_PINN\"][0][0]\n",
    "batch_WS = WS_data[\"batch_WS\"][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d3bb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(4608)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_PINN#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc5a928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.nanmax(T_WS)=np.float64(49.32331232012715) , np.nanmin(T_WS)=np.float64(0.0)\n",
      "np.nanmax(P_WS)=np.float64(7.912132117520519) , np.nanmin(P_WS)=np.float64(-4.412815666525302)\n",
      "np.nanmax(U_WS)=np.float64(0.7578370168417706) , np.nanmin(U_WS)=np.float64(-0.7057915726090886)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{np.nanmax(T_WS)=} , {np.nanmin(T_WS)=}\")\n",
    "print(f\"{np.nanmax(P_WS)=} , {np.nanmin(P_WS)=}\")\n",
    "print(f\"{np.nanmax(U_WS)=} , {np.nanmin(U_WS)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec9696b",
   "metadata": {},
   "source": [
    "# Utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d427e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda')\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"{device=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe29f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "num_epochs = 1000 # number of epochs\n",
    "lamb = 2 # Tuning of physics constraints\n",
    "# dtype = np.float64\n",
    "dtype = np.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8643311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d906c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _flatten_grid(T, X, Y, target=None):\n",
    "    \"\"\"\n",
    "    Recibe matrices 2D (N_x by N_t) y devuelve vectores columna aplanados (N_x*N_t, 1).\n",
    "    Aplica máscara de NaN con base en `target` si se provee.\n",
    "    \"\"\"\n",
    "    # Aplanar\n",
    "    t = T.reshape(-1, 1)\n",
    "    x = X.reshape(-1, 1)\n",
    "    y = Y.reshape(-1, 1)\n",
    "    tgt = None if target is None else target.reshape(-1, 1)\n",
    "\n",
    "    # Máscara de NaN según target si existe\n",
    "    if tgt is not None:\n",
    "        mask = ~np.isnan(tgt[:, 0])\n",
    "        t, x, y = t[mask], x[mask], y[mask]\n",
    "        tgt = tgt[mask]\n",
    "    return t, x, y, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a74397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_device(batch, device):\n",
    "    if isinstance(batch, (list, tuple)):\n",
    "        return [item.to(device, non_blocking=True) for item in batch]\n",
    "    elif isinstance(batch, dict):\n",
    "        return {key: val.to(device, non_blocking=True) for key, val in batch.items()}\n",
    "    else:\n",
    "        return batch.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2be6054",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b854ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset para un campo escalar (u, v o p) en las ubicaciones WS (t, x, y [,z]).\n",
    "    Permite barajar con DataLoader + RandomSampler.\n",
    "    \"\"\"\n",
    "    def __init__(self, T_WS, X_WS, Y_WS, target_WS):\n",
    "        t, x, y, tgt = _flatten_grid(T_WS, X_WS, Y_WS, target_WS)\n",
    "        self.t = torch.from_numpy(t.astype(dtype))\n",
    "        self.x = torch.from_numpy(x.astype(dtype))\n",
    "        self.y = torch.from_numpy(y.astype(dtype))\n",
    "        self.target = torch.from_numpy(tgt.astype(dtype))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.t.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return torch.concat([self.t[idx], self.x[idx], self.y[idx], self.target[idx]], axis=1)\n",
    "        return self.t[idx], self.x[idx], self.y[idx], self.target[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb1796",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSEqnRefDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset para (t, x, y) de referencia desde WS (para términos de ecuaciones que usan ref).\n",
    "    \"\"\"\n",
    "    def __init__(self, T_WS, X_WS, Y_WS):\n",
    "        t, x, y, _ = _flatten_grid(T_WS, X_WS, Y_WS)\n",
    "        self.t = torch.from_numpy(t.astype(dtype))\n",
    "        self.x = torch.from_numpy(x.astype(dtype))\n",
    "        self.y = torch.from_numpy(y.astype(dtype))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.t.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return torch.concat([self.t[idx], self.x[idx], self.y[idx]], axis=1)\n",
    "        return self.t[idx], self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0a7e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINNEqnDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset de puntos de colación (PINN) provenientes de malla PINN (sin targets).\n",
    "    \"\"\"\n",
    "    def __init__(self, T_PINN, X_PINN, Y_PINN):\n",
    "        t, x, y, _ = _flatten_grid(T_PINN, X_PINN, Y_PINN)\n",
    "        self.t = torch.from_numpy(t.astype(dtype))\n",
    "        self.x = torch.from_numpy(x.astype(dtype))\n",
    "        self.y = torch.from_numpy(y.astype(dtype))\n",
    "        # print(self.t.shape)\n",
    "        # print(self.x.shape)\n",
    "        # print(self.y.shape)\n",
    "        # print(self.t.shape[0])\n",
    "        # print(torch.concat([self.t, self.x, self.y], axis=1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.t.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return torch.concat([self.t[idx,:], self.x[idx,:], self.y[idx,:]], axis=1)\n",
    "        return self.t[idx], self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b30029",
   "metadata": {},
   "source": [
    "# Preparación de datasets y dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc705c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "dim_N_WS = X_WS.shape[0]\n",
    "dim_T_WS = X_WS.shape[1]\n",
    "dim_N_PINN = X_PINN.shape[0]\n",
    "dim_T_PINN = T_PINN.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3623e854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_PINN=4608, batch_WS=231\n"
     ]
    }
   ],
   "source": [
    "# Dimensions\n",
    "dim_N_data = dim_N_WS\n",
    "dim_T_data = dim_T_WS\n",
    "dim_T_eqns = dim_T_PINN\n",
    "dim_N_eqns = dim_N_PINN\n",
    "\n",
    "num_samples_WS = int(dim_N_data * dim_T_data)\n",
    "num_samples_PINN = int(dim_N_eqns * dim_T_eqns)\n",
    "\n",
    "batch_PINN = int(batch_PINN)\n",
    "batch_WS = int(batch_WS)\n",
    "\n",
    "print(f\"{batch_PINN=}, {batch_WS=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05228cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjuntos\n",
    "ds_u = WSDataset(T_WS, X_WS, Y_WS, U_WS)\n",
    "ds_v = WSDataset(T_WS, X_WS, Y_WS, V_WS)\n",
    "ds_p = WSDataset(T_WS, X_WS, Y_WS, P_WS)\n",
    "\n",
    "ds_eqns_ref = WSEqnRefDataset(T_WS, X_WS, Y_WS)\n",
    "ds_eqns = PINNEqnDataset(T_PINN, X_PINN, Y_PINN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975313e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestreo aleatorio por época\n",
    "sampler_u = RandomSampler(ds_u, replacement=False, num_samples=num_samples_WS)\n",
    "sampler_v = RandomSampler(ds_v, replacement=False, num_samples=num_samples_WS)\n",
    "sampler_p = RandomSampler(ds_p, replacement=False, num_samples=num_samples_WS)\n",
    "sampler_eqns_ref = RandomSampler(ds_eqns_ref, replacement=False, num_samples=num_samples_WS)\n",
    "sampler_eqns = RandomSampler(ds_eqns, replacement=False, num_samples=num_samples_PINN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad80d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de los dataloaders con el muestreo simple\n",
    "loader_u = DataLoader(ds_u, batch_size=batch_WS, sampler=sampler_u, pin_memory=True)\n",
    "loader_v = DataLoader(ds_v, batch_size=batch_WS, sampler=sampler_v, pin_memory=True)\n",
    "loader_p = DataLoader(ds_p, batch_size=batch_WS, sampler=sampler_p, pin_memory=True)\n",
    "loader_eqns_ref = DataLoader(ds_eqns_ref, batch_size=batch_WS, sampler=sampler_eqns_ref, pin_memory=True)\n",
    "loader_eqns = DataLoader(ds_eqns, batch_size=batch_PINN, sampler=sampler_eqns, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddfadf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Se cargan los batches a device\n",
    "# for loader in [loader_u,loader_v,loader_p,loader_eqns_ref,loader_eqns,]:\n",
    "#     for batch in loader:\n",
    "#         batch = move_to_device(batch, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bf2b65",
   "metadata": {},
   "source": [
    "# Capa personalizada: GammaBiasLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dd04c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GammaBiasLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Capa densa personalizada:\n",
    "      y = gamma ⊙ (W_norm x) + bias\n",
    "    - W_norm: Linear (sin bias) con Weight Normalization\n",
    "    - gamma: parámetro de escala por-neurona\n",
    "    - bias: sesgo por-neurona\n",
    "\n",
    "    Args:\n",
    "        in_features  (int): tamaño de entrada\n",
    "        out_features (int): número de unidades (neuronas)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        # Linear sin bias para emular Dense(use_bias=False)\n",
    "        linear = nn.Linear(in_features, out_features, bias=False)\n",
    "        # Inicialización uniforme [-1, 1], como en tu RandomUniform\n",
    "        nn.init.uniform_(linear.weight, a=-1.0, b=1.0)\n",
    "        \n",
    "        # Weight Normalization (equivalente a tfa.layers.WeightNormalization)\n",
    "        self.w = weight_norm(linear)  # añade weight_g y weight_v internamente\n",
    "\n",
    "        # Parámetros gamma y bias (forma [out_features])\n",
    "        self.gamma = nn.Parameter(torch.ones(out_features))\n",
    "        self.bias  = nn.Parameter(torch.zeros(out_features))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (batch, in_features) -> out: (batch, out_features)\n",
    "        y = self.w(x)\n",
    "        # Broadcasting de gamma y bias sobre la dimensión batch\n",
    "        return y * self.gamma + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7455f9c",
   "metadata": {},
   "source": [
    "# PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3fc7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PINNNet(nn.Module):\n",
    "#     def __init__(self, num_input_variables=3, num_output_variables=3):\n",
    "#         super().__init__()\n",
    "\n",
    "#         neurons = 200 * num_output_variables\n",
    "#         layers_sizes = (\n",
    "#             [num_input_variables]\n",
    "#             + (2 * (num_input_variables + num_output_variables)) * [neurons]\n",
    "#             + [num_output_variables]\n",
    "#         )\n",
    "#         # Guardamos para reproducir los mismos rangos de tu for\n",
    "#         L = layers_sizes\n",
    "#         # Índices de particiones como en tu código\n",
    "#         mid_end = 2 * int((len(L) - 2) / 3)\n",
    "\n",
    "#         # Construimos los módulos siguiendo el mismo patrón\n",
    "#         mods = []\n",
    "\n",
    "#         # Primer bloque: GammaBias(layers[1]) + tanh\n",
    "#         mods.append(GammaBiasLayer(L[0], L[1]))\n",
    "#         mods.append(nn.Tanh())\n",
    "\n",
    "#         # Bloques intermedios: for l in layers[2 : mid_end]: GammaBias(l) + tanh\n",
    "#         in_dim = L[1]\n",
    "#         for l in L[2:mid_end]:\n",
    "#             mods.append(GammaBiasLayer(in_dim, l))\n",
    "#             mods.append(nn.Tanh())\n",
    "#             in_dim = l\n",
    "\n",
    "#         # Bloques finales (antes de salida): for l in layers[mid_end : -1]:\n",
    "#         #   GammaBias(layers[-2])  (tal como en tu código original)\n",
    "#         # Esto apila capas con anchura fija igual a L[-2]\n",
    "#         penultimate = L[-2]\n",
    "#         for _ in L[mid_end:-1]:\n",
    "#             mods.append(GammaBiasLayer(in_dim, penultimate))\n",
    "#             # OJO: el original no aplicaba activación aquí\n",
    "#             in_dim = penultimate\n",
    "\n",
    "#         # Capa de salida: GammaBias(layers[-1])\n",
    "#         mods.append(GammaBiasLayer(in_dim, L[-1]))\n",
    "\n",
    "#         self.net = nn.Sequential(*mods)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x: [N, 3] -> [N, 3]  (u, v, p)\n",
    "#         return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7cf944",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class PINNNet(nn.Module):\n",
    "#     def __init__(self, num_input_variables=3, num_output_variables=3):\n",
    "#         super().__init__()\n",
    "#         neurons = 200 * num_output_variables\n",
    "#         hidden_sizes = (2 * (num_input_variables + num_output_variables))*[neurons]\n",
    "#         layers = []\n",
    "#         last = num_input_variables\n",
    "#         for h in hidden_sizes + [num_output_variables]:\n",
    "#                      layers.append(GammaBiasLayer(last, h))\n",
    "#                      last = h\n",
    "#         self.layers = nn.ModuleList(layers)\n",
    "#         self.activation = nn.Tanh()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Aplicar activación solo en las primeras 5 capas ocultas\n",
    "#         for i, layer in enumerate(self.layers):\n",
    "#             x = layer(x)\n",
    "#             if i < 8:  # activación en capas 0 a 4\n",
    "#                          x = self.activation(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effcfe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PINNNet(nn.Module):\n",
    "    def __init__(self, num_input_variables=3, num_output_variables=3):\n",
    "        super().__init__()\n",
    "        self.activation = nn.Tanh()\n",
    "        self.l01 = GammaBiasLayer(3, 600)\n",
    "        self.l02 = GammaBiasLayer(600, 600)\n",
    "        self.l03 = GammaBiasLayer(600, 600)\n",
    "        self.l04 = GammaBiasLayer(600, 600)\n",
    "        self.l05 = GammaBiasLayer(600, 600)\n",
    "        self.l06 = GammaBiasLayer(600, 600)\n",
    "        self.l07 = GammaBiasLayer(600, 600)\n",
    "        self.l08 = GammaBiasLayer(600, 600)\n",
    "        self.l09 = GammaBiasLayer(600, 600)\n",
    "        self.l10 = GammaBiasLayer(600, 600)\n",
    "        self.l11 = GammaBiasLayer(600, 600)\n",
    "        self.l12 = GammaBiasLayer(600, 600)\n",
    "        self.lfi = GammaBiasLayer(600, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        a01 = self.activation(self.l01(x))\n",
    "        a02 = self.activation(self.l02(a01))\n",
    "        a03 = self.activation(self.l03(a02))\n",
    "        a04 = self.activation(self.l04(a03))\n",
    "        a05 = self.activation(self.l05(a04))\n",
    "        a06 = self.activation(self.l06(a05))\n",
    "        a07 = self.activation(self.l07(a06))\n",
    "        a08 = self.activation(self.l08(a07))\n",
    "        a09 = self.l09(a08)\n",
    "        a10 = self.l10(a09)\n",
    "        a11 = self.l11(a10)\n",
    "        a12 = self.l12(a11)\n",
    "        afi = self.lfi(a12)\n",
    "\n",
    "\n",
    "        return afi, (a01, a02, a03, a04, a05, a06, a07, a08, a09, a10, a11, a12, afi, )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce0e027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PINNNet().to(device).double()\n",
    "model = PINNNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7eb7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  'named_parameters',\n",
    "#  'parameters',\n",
    "def info_model(model):\n",
    "    for i in model.named_parameters(): \n",
    "        print(20*\"#\")\n",
    "        print(f\"Parámetro: {i[0]:10}\")\n",
    "        print(f\"Dim parámetro: {i[1].shape}\")\n",
    "        print(f\"Dim parámetro: {i[1][:5]}\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aa7805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Parámetro: l01.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l01.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l01.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[1.1537],\n",
      "        [0.6830],\n",
      "        [1.0407],\n",
      "        [0.9750],\n",
      "        [1.0204]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l01.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 3])\n",
      "Dim parámetro: tensor([[ 0.6559,  0.6975, -0.6437],\n",
      "        [ 0.5295,  0.1893,  0.3877],\n",
      "        [ 0.5463, -0.5104,  0.7239],\n",
      "        [-0.0981, -0.4058,  0.8811],\n",
      "        [ 0.8861, -0.3403, -0.3746]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l02.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l02.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l02.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.4389],\n",
      "        [13.5673],\n",
      "        [14.6712],\n",
      "        [13.7905],\n",
      "        [14.6239]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l02.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[ 0.6871,  0.9561,  0.1057,  ..., -0.2759,  0.2907,  0.3948],\n",
      "        [ 0.6835,  0.0776, -0.5742,  ...,  0.0057, -0.1256, -0.0636],\n",
      "        [ 0.6685, -0.5220,  0.6241,  ...,  0.9472, -0.7029,  0.2562],\n",
      "        [ 0.8483, -0.5695, -0.1631,  ...,  0.8482,  0.4483, -0.7179],\n",
      "        [ 0.5217,  0.5519,  0.9914,  ...,  0.8378,  0.8397,  0.4180]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l03.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l03.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l03.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.2694],\n",
      "        [14.0656],\n",
      "        [13.8109],\n",
      "        [14.1541],\n",
      "        [13.5901]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l03.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.4708, -0.6973, -0.3287,  ...,  0.1222, -0.6993, -0.5033],\n",
      "        [-0.1913,  0.2383,  0.7957,  ..., -0.8634,  0.0478,  0.4068],\n",
      "        [-0.6836, -0.3484,  0.9402,  ...,  0.1655,  0.7878, -0.1465],\n",
      "        [-0.6792,  0.5930, -0.1439,  ..., -0.1030, -0.7344,  0.3552],\n",
      "        [ 0.7082,  0.0383, -0.8814,  ..., -0.1800,  0.1463, -0.2389]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l04.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l04.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l04.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[13.7767],\n",
      "        [14.5479],\n",
      "        [14.0371],\n",
      "        [13.9903],\n",
      "        [14.5850]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l04.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.2132, -0.3289, -0.6671,  ...,  0.2703,  0.6832, -0.6609],\n",
      "        [ 0.4969,  0.1876, -0.5401,  ..., -0.7251,  0.1291, -0.7737],\n",
      "        [-0.4815, -0.6621, -0.0262,  ...,  0.2020, -0.1731, -0.7846],\n",
      "        [ 0.6913, -0.8229,  0.5706,  ..., -0.8525,  0.1842,  0.3531],\n",
      "        [ 0.8193,  0.8788, -0.7583,  ..., -0.2512, -0.4285, -0.9829]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l05.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l05.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l05.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.3453],\n",
      "        [14.2198],\n",
      "        [14.0731],\n",
      "        [14.3287],\n",
      "        [14.4587]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l05.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.5535, -0.6388, -0.9791,  ...,  0.2012, -0.2136,  0.1824],\n",
      "        [ 0.7800,  0.4213,  0.5066,  ..., -0.8905,  0.9592, -0.9390],\n",
      "        [ 0.0546, -0.5997,  0.2778,  ...,  0.7730, -0.0212,  0.8454],\n",
      "        [-0.0962, -0.1040, -0.9505,  ..., -0.0395, -0.7370, -0.4685],\n",
      "        [-0.2736, -0.0892, -0.9661,  ..., -0.7093, -0.0057, -0.6542]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l06.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l06.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l06.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.2913],\n",
      "        [13.9985],\n",
      "        [14.3805],\n",
      "        [14.1601],\n",
      "        [14.3357]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l06.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[ 0.6443,  0.6010, -0.9351,  ...,  0.8518,  0.0367, -0.1698],\n",
      "        [ 0.9864, -0.2879,  0.6890,  ...,  0.2745,  0.5200,  0.7622],\n",
      "        [ 0.8547,  0.9969,  0.9620,  ..., -0.7654, -0.7354, -0.8518],\n",
      "        [-0.2772,  0.3475, -0.0488,  ...,  0.5700, -0.4006,  0.1645],\n",
      "        [-0.6994, -0.4046,  0.8345,  ..., -0.9519, -0.2276, -0.6924]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l07.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l07.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l07.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.4121],\n",
      "        [13.9767],\n",
      "        [13.8591],\n",
      "        [14.0035],\n",
      "        [13.8459]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l07.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.5673,  0.4738,  0.3427,  ..., -0.2057, -0.9854,  0.0426],\n",
      "        [ 0.5489, -0.0853, -0.7776,  ..., -0.3139,  0.9274,  0.3501],\n",
      "        [-0.9789, -0.3269, -0.3954,  ...,  0.5816, -0.6826, -0.3930],\n",
      "        [-0.8702, -0.3840, -0.8146,  ...,  0.6614, -0.9949,  0.0586],\n",
      "        [-0.0821, -0.9500,  0.9391,  ..., -0.6365, -0.0804, -0.0154]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l08.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l08.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l08.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.1317],\n",
      "        [14.3957],\n",
      "        [14.2970],\n",
      "        [13.8194],\n",
      "        [13.9807]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l08.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.0684,  0.5977,  0.8942,  ...,  0.3460, -0.9562, -0.2035],\n",
      "        [ 0.6116, -0.7567,  0.9228,  ..., -0.3851,  0.2864, -0.7013],\n",
      "        [-0.0917, -0.5360, -0.9290,  ..., -0.6399, -0.2008,  0.2293],\n",
      "        [ 0.5559,  0.8348, -0.4002,  ...,  0.2520,  0.5939, -0.2458],\n",
      "        [ 0.9462,  0.7383,  0.4689,  ...,  0.6315,  0.5102,  0.6758]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l09.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l09.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l09.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.0965],\n",
      "        [14.1900],\n",
      "        [13.9275],\n",
      "        [14.1550],\n",
      "        [13.7451]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l09.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.9229,  0.6505,  0.1473,  ...,  0.5044,  0.6742, -0.7342],\n",
      "        [-0.4551, -0.7742,  0.8113,  ...,  0.5573,  0.8533,  0.7311],\n",
      "        [ 0.8350, -0.6763, -0.8058,  ...,  0.6354,  0.5372,  0.4186],\n",
      "        [ 0.8052, -0.0519, -0.2502,  ...,  0.3148,  0.3226, -0.4012],\n",
      "        [-0.7149,  0.1576,  0.1890,  ...,  0.9033,  0.8529, -0.1539]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l10.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l10.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l10.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.1593],\n",
      "        [13.8843],\n",
      "        [14.2610],\n",
      "        [14.6419],\n",
      "        [14.1405]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l10.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.2141,  0.0182,  0.2382,  ...,  0.7561,  0.6285, -0.8943],\n",
      "        [ 0.8400,  0.2460,  0.1895,  ..., -0.8183, -0.5697, -0.2678],\n",
      "        [ 0.5407, -0.6177, -0.8056,  ..., -0.2077,  0.1404, -0.5982],\n",
      "        [-0.5825,  0.4528,  0.3377,  ..., -0.9031, -0.5521, -0.9283],\n",
      "        [ 0.4996,  0.9934,  0.3813,  ..., -0.6539, -0.0783, -0.9421]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l11.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l11.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l11.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[13.9311],\n",
      "        [14.0192],\n",
      "        [14.3504],\n",
      "        [13.7961],\n",
      "        [14.0364]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l11.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-1.0252e-01, -8.4855e-01, -8.6159e-01,  ..., -1.6484e-01,\n",
      "          3.4293e-01,  7.5845e-01],\n",
      "        [ 9.2328e-04,  7.6889e-01,  1.3378e-01,  ...,  8.0115e-01,\n",
      "         -1.5561e-01,  4.2724e-02],\n",
      "        [-1.4773e-01,  8.4083e-01,  9.9066e-01,  ..., -7.6634e-01,\n",
      "         -4.2618e-01, -5.1494e-01],\n",
      "        [ 2.4558e-01, -2.0621e-01,  9.5098e-01,  ..., -6.2394e-01,\n",
      "         -5.7332e-01, -7.3193e-01],\n",
      "        [ 8.7902e-01, -6.9896e-01, -8.2742e-01,  ..., -7.1601e-01,\n",
      "         -4.5050e-01, -3.9147e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l12.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l12.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l12.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.3955],\n",
      "        [14.7006],\n",
      "        [14.1305],\n",
      "        [14.0103],\n",
      "        [14.1039]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l12.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[ 0.3990, -0.1525, -0.8500,  ...,  0.1287,  0.4216, -0.8424],\n",
      "        [-0.7158,  0.8529, -0.2187,  ..., -0.9610,  0.3691, -0.6698],\n",
      "        [ 0.2980,  0.7911,  0.4784,  ..., -0.4754, -0.3311, -0.8379],\n",
      "        [-0.9810,  0.8042,  0.6447,  ..., -0.3274,  0.4195,  0.7202],\n",
      "        [-0.0192, -0.9565, -0.9786,  ..., -0.8798,  0.8914,  0.0606]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: lfi.gamma \n",
      "Dim parámetro: torch.Size([3])\n",
      "Dim parámetro: tensor([1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: lfi.bias  \n",
      "Dim parámetro: torch.Size([3])\n",
      "Dim parámetro: tensor([0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: lfi.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([3, 1])\n",
      "Dim parámetro: tensor([[14.4558],\n",
      "        [13.9026],\n",
      "        [14.4056]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: lfi.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([3, 600])\n",
      "Dim parámetro: tensor([[-0.5025, -0.5699,  0.5540,  ...,  0.5311,  0.1282,  0.2799],\n",
      "        [-0.3356, -0.5155,  0.3135,  ..., -0.9594, -0.1095, -0.3112],\n",
      "        [ 0.4784,  0.5623,  0.0107,  ..., -0.8606, -0.8373, -0.7438]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751a631d",
   "metadata": {},
   "source": [
    "# Funciones de perdidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7686f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = nn.MSELoss()\n",
    "# mse_loss = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "# @torch.enable_grad()\n",
    "# def loss_NS_2D(model, t_eqns, x_eqns, y_eqns):\n",
    "def loss_NS_2D(model, t_eqns, x_eqns, y_eqns):\n",
    "    \"\"\"\n",
    "    Calcula los residuales 2D (incompresible) aproximados:\n",
    "      e1 = u_x + v_y\n",
    "      e2 = u_t + (u u_x + v u_y) + p_x\n",
    "      e3 = v_t + (u v_x + v v_y) + p_y\n",
    "    Devuelve MSE(0, e1) + MSE(0, e2) + MSE(0, e3)\n",
    "    \"\"\"\n",
    "    # Asegurar gradientes con respecto a entradas\n",
    "    for ten in (t_eqns, x_eqns, y_eqns):\n",
    "        ten.requires_grad_(True)\n",
    "\n",
    "    X = torch.cat([t_eqns, x_eqns, y_eqns], dim=1)  # [N, 3]\n",
    "    Y, actv = model(X)                                     # [N, 3]\n",
    "    u, v, p = Y[:, 0:1], Y[:, 1:2], Y[:, 2:3]\n",
    "\n",
    "    ones_u = torch.ones_like(u)\n",
    "    ones_v = torch.ones_like(v)\n",
    "    ones_p = torch.ones_like(p)\n",
    "\n",
    "    # Derivadas primeras\n",
    "    u_t = torch.autograd.grad(u, t_eqns, grad_outputs=ones_u, create_graph=True)[0]\n",
    "    v_t = torch.autograd.grad(v, t_eqns, grad_outputs=ones_v, create_graph=True)[0]\n",
    "\n",
    "    u_x = torch.autograd.grad(u, x_eqns, grad_outputs=ones_u, create_graph=True)[0]\n",
    "    v_x = torch.autograd.grad(v, x_eqns, grad_outputs=ones_v, create_graph=True)[0]\n",
    "    p_x = torch.autograd.grad(p, x_eqns, grad_outputs=ones_p, create_graph=True)[0]\n",
    "\n",
    "    u_y = torch.autograd.grad(u, y_eqns, grad_outputs=ones_u, create_graph=True)[0]\n",
    "    v_y = torch.autograd.grad(v, y_eqns, grad_outputs=ones_v, create_graph=True)[0]\n",
    "    p_y = torch.autograd.grad(p, y_eqns, grad_outputs=ones_p, create_graph=True)[0]\n",
    "\n",
    "    # Residuales Navier–Stokes (sin términos viscosos, conforme a tu snippet)\n",
    "    e1 = (u_x + v_y)\n",
    "    e2 = (u_t + (u * u_x + v * u_y) + p_x)\n",
    "    e3 = (v_t + (u * v_x + v * v_y) + p_y)\n",
    "\n",
    "    zero = torch.zeros_like(e1)\n",
    "    return (\n",
    "        mse_loss(e1, zero) +\n",
    "        mse_loss(e2, zero) +\n",
    "        mse_loss(e3, zero)\n",
    "    )\n",
    "\n",
    "# @torch.no_grad()\n",
    "def _safe_std(x: torch.Tensor, eps: float = 1e-8):\n",
    "    # std con protección para divisiones por cero\n",
    "    s = torch.std(x)\n",
    "    return s.clamp_min(eps)\n",
    "\n",
    "# def loss_u(t_b, x_b, y_b, u_b):\n",
    "def loss_u(model, t_b, x_b, y_b, u_b):\n",
    "    X = torch.cat([t_b, x_b, y_b], dim=1)\n",
    "    Y, actv = model(X)\n",
    "    u_pred = Y[:, 0:1]\n",
    "    for i, a in enumerate(actv):\n",
    "        print(f\"Capa {i:02d}: shape={tuple(a.shape)}, mean={a.mean():+.4f}, std={a.std():.4f}\")\n",
    "    # print(f\"{X[:5,:]=}\")\n",
    "    # print(f\"{Y[:5,:]=}\")\n",
    "    # print(f\"{_safe_std(u_b) ** 2:=.2f}\")\n",
    "    return mse_loss(u_pred, u_b) / (_safe_std(u_b) ** 2)\n",
    "\n",
    "# def loss_v(t_b, x_b, y_b, v_b):\n",
    "def loss_v(model, t_b, x_b, y_b, v_b):\n",
    "    X = torch.cat([t_b, x_b, y_b], dim=1)\n",
    "    Y, actv = model(X)\n",
    "    v_pred = Y[:, 1:2]\n",
    "    return mse_loss(v_pred, v_b) / (_safe_std(v_b) ** 2)\n",
    "\n",
    "# def loss_p(t_b, x_b, y_b, p_b):\n",
    "def loss_p(model, t_b, x_b, y_b, p_b):\n",
    "    X = torch.cat([t_b, x_b, y_b], dim=1)\n",
    "    Y, actv = model(X)\n",
    "    p_pred = Y[:, 2:3]\n",
    "    return mse_loss(p_pred, p_b) / (_safe_std(p_b) ** 2)\n",
    "\n",
    "def loss_total(\n",
    "    model,\n",
    "    # datos u\n",
    "    t_u_b, x_u_b, y_u_b, u_u_b,\n",
    "    # datos v\n",
    "    t_v_b, x_v_b, y_v_b, v_v_b,\n",
    "    # datos p\n",
    "    t_p_b, x_p_b, y_p_b, p_p_b,\n",
    "    # ecuaciones (referencia + ecuaciones)\n",
    "    t_eqns_ref_b, x_eqns_ref_b, y_eqns_ref_b,\n",
    "    t_eqns_b, x_eqns_b, y_eqns_b,\n",
    "    lamb: float\n",
    "):\n",
    "    NS_eqns = lamb * loss_NS_2D(model, t_eqns_b,     x_eqns_b,     y_eqns_b,   )\n",
    "    NS_data = lamb * loss_NS_2D(model, t_eqns_ref_b, x_eqns_ref_b, y_eqns_ref_b)\n",
    "    P_e = loss_p(model, t_p_b, x_p_b, y_p_b, p_p_b)\n",
    "    U_e = loss_u(model, t_u_b, x_u_b, y_u_b, u_u_b)\n",
    "    V_e = loss_v(model, t_v_b, x_v_b, y_v_b, v_v_b)\n",
    "\n",
    "    total_e = NS_eqns + NS_data + U_e + V_e + P_e\n",
    "\n",
    "    return (NS_eqns ** 2 + NS_data ** 2 + U_e ** 2 + V_e ** 2 + P_e ** 2) / total_e\n",
    "    # return total_e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a3f9a5",
   "metadata": {},
   "source": [
    "# Gradiente y optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594f5251",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48910f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def grad(model,\n",
    "#          t_u_batch, x_u_batch, y_u_batch, u_u_batch,\n",
    "#          t_v_batch, x_v_batch, y_v_batch, v_v_batch,\n",
    "#          t_p_batch, x_p_batch, y_p_batch, p_p_batch,\n",
    "#          t_eqns_ref_batch, x_eqns_ref_batch, y_eqns_ref_batch,\n",
    "#          t_eqns_batch, x_eqns_batch, y_eqns_batch,\n",
    "#          lamb):\n",
    "    \n",
    "#     model.train()  # Asegura que el modelo esté en modo entrenamiento\n",
    "#     model.zero_grad()  # Limpia gradientes anteriores\n",
    "\n",
    "#     # Forward pass\n",
    "#     loss_value = loss_total(model,\n",
    "#                             t_u_batch, x_u_batch, y_u_batch, u_u_batch,\n",
    "#                             t_v_batch, x_v_batch, y_v_batch, v_v_batch,\n",
    "#                             t_p_batch, x_p_batch, y_p_batch, p_p_batch,\n",
    "#                             t_eqns_ref_batch, x_eqns_ref_batch, y_eqns_ref_batch,\n",
    "#                             t_eqns_batch, x_eqns_batch, y_eqns_batch,\n",
    "#                             lamb,\n",
    "#                             training=True)\n",
    "\n",
    "#     # Backward pass\n",
    "#     loss_value.backward()\n",
    "\n",
    "#     # Extraer gradientes\n",
    "#     gradient_model = [p.grad.clone() if p.grad is not None else None\n",
    "#                       for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "#     return loss_value.detach(), gradient_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d925d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "# scaler = torch.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
    "# scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
    "\n",
    "def grad_amp(model, model_optimizer, scaler,\n",
    "             t_u_batch, x_u_batch, y_u_batch, u_u_batch,\n",
    "             t_v_batch, x_v_batch, y_v_batch, v_v_batch,\n",
    "             t_p_batch, x_p_batch, y_p_batch, p_p_batch,\n",
    "             t_eqns_ref_batch, x_eqns_ref_batch, y_eqns_ref_batch,\n",
    "             t_eqns_batch, x_eqns_batch, y_eqns_batch,\n",
    "             lamb):\n",
    "    # model.train()\n",
    "    # model_optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # with torch.cuda.amp.autocast(enabled=False):\n",
    "    # with torch.cuda.amp.autocast(dtype=torch.float64):\n",
    "    # with torch.amp.autocast(dtype=torch.float64, device_type=device.type):\n",
    "    # use_amp = (device.type == \"cuda\")\n",
    "    use_amp = False\n",
    "    with torch.amp.autocast(device_type=device.type, enabled=use_amp):\n",
    "        loss_value = loss_total(model,\n",
    "                                t_u_batch, x_u_batch, y_u_batch, u_u_batch,\n",
    "                                t_v_batch, x_v_batch, y_v_batch, v_v_batch,\n",
    "                                t_p_batch, x_p_batch, y_p_batch, p_p_batch,\n",
    "                                t_eqns_ref_batch, x_eqns_ref_batch, y_eqns_ref_batch,\n",
    "                                t_eqns_batch, x_eqns_batch, y_eqns_batch,\n",
    "                                lamb)\n",
    "\n",
    "    # Escalar la pérdida antes del backward\n",
    "    scaler.scale(loss_value).backward()\n",
    "\n",
    "    # Revisa gradientes ANTES del step\n",
    "    all_finite = True\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.grad is None: \n",
    "            continue\n",
    "        if not torch.isfinite(p.grad).all():\n",
    "            print(f\"[WARN] grad no finito en {n}\")\n",
    "            all_finite = False\n",
    "            break\n",
    "\n",
    "    # (Opcional) Clipping y chequeo de gradientes\n",
    "    scaler.unscale_(model_optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # Paso del optimizador de forma segura\n",
    "    scaler.step(model_optimizer)\n",
    "\n",
    "    # Actualiza el factor de escala automáticamente\n",
    "    scaler.update()\n",
    "\n",
    "    # # Si quieres inspeccionar gradientes:\n",
    "    # grads = [p.grad.detach().clone() if p.grad is not None else None\n",
    "    #          for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "    return loss_value.detach()#, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b26ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def grad_full_double(model, model_optimizer,\n",
    "#              t_u_batch, x_u_batch, y_u_batch, u_u_batch,\n",
    "#              t_v_batch, x_v_batch, y_v_batch, v_v_batch,\n",
    "#              t_p_batch, x_p_batch, y_p_batch, p_p_batch,\n",
    "#              t_eqns_ref_batch, x_eqns_ref_batch, y_eqns_ref_batch,\n",
    "#              t_eqns_batch, x_eqns_batch, y_eqns_batch,\n",
    "#              lamb):\n",
    "#     model.train()\n",
    "#     model_optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "\n",
    "#     loss_value = loss_total(model,\n",
    "#                             t_u_batch, x_u_batch, y_u_batch, u_u_batch,\n",
    "#                             t_v_batch, x_v_batch, y_v_batch, v_v_batch,\n",
    "#                             t_p_batch, x_p_batch, y_p_batch, p_p_batch,\n",
    "#                             t_eqns_ref_batch, x_eqns_ref_batch, y_eqns_ref_batch,\n",
    "#                             t_eqns_batch, x_eqns_batch, y_eqns_batch,\n",
    "#                             lamb)\n",
    "\n",
    "#     loss_value.backward()\n",
    "#     model_optimizer.step()\n",
    "\n",
    "\n",
    "#     return loss_value.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa5841",
   "metadata": {},
   "source": [
    "# Helpers de métricas simples por época (media acumulada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e114d34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningMean:\n",
    "    def __init__(self):\n",
    "        self.sum = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        # value puede ser tensor o float\n",
    "        v = float(value) if torch.is_tensor(value) else float(value)\n",
    "        self.sum += v * n\n",
    "        self.count += n\n",
    "\n",
    "    @property\n",
    "    def result(self):\n",
    "        return self.sum / max(self.count, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9170e426",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c6b343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c3436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch_loss):\n",
    "    if epoch_loss > 1e-1:\n",
    "        new_lr = 1e-3\n",
    "    elif epoch_loss > 3e-2:\n",
    "        new_lr = 1e-4\n",
    "    elif epoch_loss > 3e-3:\n",
    "        new_lr = 1e-5\n",
    "    else:\n",
    "        new_lr = 1e-6\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "\n",
    "    return new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd29282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader_u, loader_v, loader_p, loader_eqns_ref, loader_eqns):\n",
    "\n",
    "    for idx,(\\\n",
    "        (t_u_b, x_u_b, y_u_b, u_u_b), \\\n",
    "        (t_v_b, x_v_b, y_v_b, v_v_b), \\\n",
    "        (t_p_b, x_p_b, y_p_b, p_p_b), \\\n",
    "        (t_eq_ref_b, x_eq_ref_b, y_eq_ref_b), \\\n",
    "        (t_eq_b, x_eq_b, y_eq_b)) in enumerate(zip(loader_u, loader_v, loader_p, loader_eqns_ref, loader_eqns)):\n",
    "        \n",
    "\n",
    "        # Se envian tensores a device\n",
    "        t_u_b = t_u_b.to(device)\n",
    "        x_u_b = x_u_b.to(device)\n",
    "        y_u_b = y_u_b.to(device)\n",
    "        u_u_b = u_u_b.to(device)\n",
    "        t_v_b = t_v_b.to(device)\n",
    "        x_v_b = x_v_b.to(device)\n",
    "        y_v_b = y_v_b.to(device)\n",
    "        v_v_b = v_v_b.to(device)\n",
    "        t_p_b = t_p_b.to(device)\n",
    "        x_p_b = x_p_b.to(device)\n",
    "        y_p_b = y_p_b.to(device)\n",
    "        p_p_b = p_p_b.to(device)\n",
    "        t_eq_ref_b = t_eq_ref_b.to(device)\n",
    "        x_eq_ref_b = x_eq_ref_b.to(device)\n",
    "        y_eq_ref_b = y_eq_ref_b.to(device)\n",
    "        t_eq_b = t_eq_b.to(device)\n",
    "        x_eq_b = x_eq_b.to(device)\n",
    "        y_eq_b = y_eq_b.to(device)\n",
    "\n",
    "\n",
    "        model_optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        use_amp = False\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=use_amp):\n",
    "            loss_value = loss_total(model, t_u_b, x_u_b, y_u_b, u_u_b,\n",
    "                                    t_v_b, x_v_b, y_v_b, v_v_b,\n",
    "                                    t_p_b, x_p_b, y_p_b, p_p_b,\n",
    "                                    t_eq_ref_b, x_eq_ref_b, y_eq_ref_b,\n",
    "                                    t_eq_b, x_eq_b, y_eq_b,\n",
    "                                    lamb)\n",
    "\n",
    "        # Escalar la pérdida antes del backward\n",
    "        scaler.scale(loss_value).backward()\n",
    "\n",
    "        # Revisa gradientes ANTES del step\n",
    "        all_finite = True\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.grad is None: \n",
    "                continue\n",
    "            if not torch.isfinite(p.grad).all():\n",
    "                print(f\"[WARN] grad no finito en {n}\")\n",
    "                all_finite = False\n",
    "                break\n",
    "\n",
    "        # (Opcional) Clipping y chequeo de gradientes\n",
    "        scaler.unscale_(model_optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Paso del optimizador de forma segura\n",
    "        scaler.step(model_optimizer)\n",
    "\n",
    "        # Actualiza el factor de escala automáticamente\n",
    "        scaler.update()\n",
    "        \n",
    "        return loss_value\n",
    "\n",
    "\n",
    "    #     with torch.enable_grad():\n",
    "    #         loss_train = grad_amp(model, model_optimizer, scaler,\n",
    "    #                               t_u_b, x_u_b, y_u_b, u_u_b,\n",
    "    #                               t_v_b, x_v_b, y_v_b, v_v_b,\n",
    "    #                               t_p_b, x_p_b, y_p_b, p_p_b,\n",
    "    #                               t_eq_ref_b, x_eq_ref_b, y_eq_ref_b,\n",
    "    #                               t_eq_b, x_eq_b, y_eq_b, lamb)\n",
    "    #         epoch_loss_avg.update(loss_train)\n",
    "\n",
    "    #     # print(\"\\n=== GRADIENTS ===\")\n",
    "    #     # for name, param in model.named_parameters():\n",
    "    #     #     if param.grad is not None:\n",
    "    #     #         print(f\"{name:20s} | grad.mean={param.grad.mean():+.4e} | grad.std={param.grad.std():.4e}\")\n",
    "    #     #     else:\n",
    "    #     #         print(f\"{name:20s} | grad=None\")\n",
    "\n",
    "    #     model.eval()\n",
    "    #     # with torch.no_grad():\n",
    "    #     NS_loss = loss_NS_2D(model, t_eq_b, x_eq_b, y_eq_b)\n",
    "    #     U_loss  = loss_u(model, t_u_b, x_u_b, y_u_b, u_u_b)\n",
    "    #     V_loss  = loss_v(model, t_v_b, x_v_b, y_v_b, v_v_b)\n",
    "    #     P_loss  = loss_p(model, t_p_b, x_p_b, y_p_b, p_p_b)\n",
    "\n",
    "    #     epoch_NS_loss_avg.update(NS_loss)\n",
    "    #     epoch_U_loss_avg.update(U_loss)\n",
    "    #     epoch_V_loss_avg.update(V_loss)\n",
    "    #     epoch_P_loss_avg.update(P_loss)\n",
    "\n",
    "    # return (\n",
    "    #     epoch_loss_avg.result,\n",
    "    #     epoch_NS_loss_avg.result,\n",
    "    #     epoch_U_loss_avg.result,\n",
    "    #     epoch_V_loss_avg.result,\n",
    "    #     epoch_P_loss_avg.result,\n",
    "    #     )\n",
    "        \n",
    "        \n",
    "    #     # # End epoch\n",
    "    #     # epoch_loss_avg.update(loss_train)\n",
    "    #     # epoch_NS_loss_avg.update(NS_loss)\n",
    "    #     # epoch_P_loss_avg.update(P_loss)\n",
    "    #     # epoch_U_loss_avg.update(U_loss)\n",
    "    #     # epoch_V_loss_avg.update(V_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a915ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(loader_u, loader_v, loader_p, loader_eqns_ref, loader_eqns):\n",
    "    for idx,(\\\n",
    "        (t_u_b, x_u_b, y_u_b, u_u_b), \\\n",
    "        (t_v_b, x_v_b, y_v_b, v_v_b), \\\n",
    "        (t_p_b, x_p_b, y_p_b, p_p_b), \\\n",
    "        (t_eq_ref_b, x_eq_ref_b, y_eq_ref_b), \\\n",
    "        (t_eq_b, x_eq_b, y_eq_b)) in enumerate(zip(loader_u, loader_v, loader_p, loader_eqns_ref, loader_eqns)):\n",
    "\n",
    "        # Se envian tensores a device\n",
    "        t_u_b = t_u_b.to(device)\n",
    "        x_u_b = x_u_b.to(device)\n",
    "        y_u_b = y_u_b.to(device)\n",
    "        u_u_b = u_u_b.to(device)\n",
    "        t_v_b = t_v_b.to(device)\n",
    "        x_v_b = x_v_b.to(device)\n",
    "        y_v_b = y_v_b.to(device)\n",
    "        v_v_b = v_v_b.to(device)\n",
    "        t_p_b = t_p_b.to(device)\n",
    "        x_p_b = x_p_b.to(device)\n",
    "        y_p_b = y_p_b.to(device)\n",
    "        p_p_b = p_p_b.to(device)\n",
    "        t_eq_ref_b = t_eq_ref_b.to(device)\n",
    "        x_eq_ref_b = x_eq_ref_b.to(device)\n",
    "        y_eq_ref_b = y_eq_ref_b.to(device)\n",
    "        t_eq_b = t_eq_b.to(device)\n",
    "        x_eq_b = x_eq_b.to(device)\n",
    "        y_eq_b = y_eq_b.to(device)\n",
    "        \n",
    "        NS_loss = loss_NS_2D(model, t_eq_b, x_eq_b, y_eq_b)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            \n",
    "            U_loss  = loss_u(model, t_u_b, x_u_b, y_u_b, u_u_b)\n",
    "            V_loss  = loss_v(model, t_v_b, x_v_b, y_v_b, v_v_b)\n",
    "            P_loss  = loss_p(model, t_p_b, x_p_b, y_p_b, p_p_b)\n",
    "            \n",
    "        return NS_loss, U_loss, V_loss, P_loss\n",
    "            \n",
    "\n",
    "        # # Escalar la pérdida antes del backward\n",
    "        # scaler.scale(loss_value).backward()\n",
    "\n",
    "        # # Revisa gradientes ANTES del step\n",
    "        # all_finite = True\n",
    "        # for n, p in model.named_parameters():\n",
    "        #     if p.grad is None: \n",
    "        #         continue\n",
    "        #     if not torch.isfinite(p.grad).all():\n",
    "        #         print(f\"[WARN] grad no finito en {n}\")\n",
    "        #         all_finite = False\n",
    "        #         break\n",
    "\n",
    "        # # (Opcional) Clipping y chequeo de gradientes\n",
    "        # scaler.unscale_(model_optimizer)\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # # Paso del optimizador de forma segura\n",
    "        # scaler.step(model_optimizer)\n",
    "\n",
    "        # # Actualiza el factor de escala automáticamente\n",
    "        # scaler.update()\n",
    "        \n",
    "        # return loss_value\n",
    "\n",
    "\n",
    "    #     with torch.enable_grad():\n",
    "    #         loss_train = grad_amp(model, model_optimizer, scaler,\n",
    "    #                               t_u_b, x_u_b, y_u_b, u_u_b,\n",
    "    #                               t_v_b, x_v_b, y_v_b, v_v_b,\n",
    "    #                               t_p_b, x_p_b, y_p_b, p_p_b,\n",
    "    #                               t_eq_ref_b, x_eq_ref_b, y_eq_ref_b,\n",
    "    #                               t_eq_b, x_eq_b, y_eq_b, lamb)\n",
    "    #         epoch_loss_avg.update(loss_train)\n",
    "\n",
    "    #     # print(\"\\n=== GRADIENTS ===\")\n",
    "    #     # for name, param in model.named_parameters():\n",
    "    #     #     if param.grad is not None:\n",
    "    #     #         print(f\"{name:20s} | grad.mean={param.grad.mean():+.4e} | grad.std={param.grad.std():.4e}\")\n",
    "    #     #     else:\n",
    "    #     #         print(f\"{name:20s} | grad=None\")\n",
    "\n",
    "    #     model.eval()\n",
    "    #     # with torch.no_grad():\n",
    "    #     NS_loss = loss_NS_2D(model, t_eq_b, x_eq_b, y_eq_b)\n",
    "    #     U_loss  = loss_u(model, t_u_b, x_u_b, y_u_b, u_u_b)\n",
    "    #     V_loss  = loss_v(model, t_v_b, x_v_b, y_v_b, v_v_b)\n",
    "    #     P_loss  = loss_p(model, t_p_b, x_p_b, y_p_b, p_p_b)\n",
    "\n",
    "    #     epoch_NS_loss_avg.update(NS_loss)\n",
    "    #     epoch_U_loss_avg.update(U_loss)\n",
    "    #     epoch_V_loss_avg.update(V_loss)\n",
    "    #     epoch_P_loss_avg.update(P_loss)\n",
    "\n",
    "    # return (\n",
    "    #     epoch_loss_avg.result,\n",
    "    #     epoch_NS_loss_avg.result,\n",
    "    #     epoch_U_loss_avg.result,\n",
    "    #     epoch_V_loss_avg.result,\n",
    "    #     epoch_P_loss_avg.result,\n",
    "    #     )\n",
    "        \n",
    "        \n",
    "    #     # # End epoch\n",
    "    #     # epoch_loss_avg.update(loss_train)\n",
    "    #     # epoch_NS_loss_avg.update(NS_loss)\n",
    "    #     # epoch_P_loss_avg.update(P_loss)\n",
    "    #     # epoch_U_loss_avg.update(U_loss)\n",
    "    #     # epoch_V_loss_avg.update(V_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab17f351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capa 00: shape=(231, 600), mean=-0.0094, std=0.9423\n",
      "Capa 01: shape=(231, 600), mean=-0.0051, std=0.9697\n",
      "Capa 02: shape=(231, 600), mean=-0.0410, std=0.9701\n",
      "Capa 03: shape=(231, 600), mean=-0.0483, std=0.9674\n",
      "Capa 04: shape=(231, 600), mean=+0.0121, std=0.9703\n",
      "Capa 05: shape=(231, 600), mean=-0.0325, std=0.9708\n",
      "Capa 06: shape=(231, 600), mean=+0.0306, std=0.9704\n",
      "Capa 07: shape=(231, 600), mean=-0.0172, std=0.9707\n",
      "Capa 08: shape=(231, 600), mean=+0.0507, std=13.7149\n",
      "Capa 09: shape=(231, 600), mean=-5.6684, std=192.8243\n",
      "Capa 10: shape=(231, 600), mean=-49.7654, std=2714.3218\n",
      "Capa 11: shape=(231, 600), mean=-477.4462, std=38886.5352\n",
      "Capa 12: shape=(231, 3), mean=+115328.5078, std=533392.9375\n",
      "[WARN] grad no finito en l01.gamma\n",
      "Capa 00: shape=(231, 600), mean=-0.0098, std=0.9512\n",
      "Capa 01: shape=(231, 600), mean=-0.0030, std=0.9713\n",
      "Capa 02: shape=(231, 600), mean=-0.0429, std=0.9705\n",
      "Capa 03: shape=(231, 600), mean=-0.0485, std=0.9671\n",
      "Capa 04: shape=(231, 600), mean=+0.0104, std=0.9705\n",
      "Capa 05: shape=(231, 600), mean=-0.0320, std=0.9710\n",
      "Capa 06: shape=(231, 600), mean=+0.0330, std=0.9703\n",
      "Capa 07: shape=(231, 600), mean=-0.0124, std=0.9710\n",
      "Capa 08: shape=(231, 600), mean=+0.0412, std=13.7164\n",
      "Capa 09: shape=(231, 600), mean=-7.3189, std=192.7952\n",
      "Capa 10: shape=(231, 600), mean=-29.5941, std=2713.7842\n",
      "Capa 11: shape=(231, 600), mean=-179.1262, std=38904.4414\n",
      "Capa 12: shape=(231, 3), mean=+91884.3672, std=523835.5625\n",
      "Epoch:    1 | Loss training:        inf | NS_Loss: 2.0967e+28 | U_Loss: 6.4199e+12 | V_Loss: 1.3612e+13 | P_Loss: 2.7701e+10 | learning rate: 1.0000e-03 | \n",
      "\n",
      "=== GRADIENTS ===\n",
      "l01.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l01.bias             | grad.mean=+nan | grad.std=nan\n",
      "l01.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l01.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l02.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l02.bias             | grad.mean=+nan | grad.std=nan\n",
      "l02.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l02.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l03.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l03.bias             | grad.mean=+nan | grad.std=nan\n",
      "l03.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l03.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l04.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l04.bias             | grad.mean=+nan | grad.std=nan\n",
      "l04.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l04.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l05.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l05.bias             | grad.mean=+nan | grad.std=nan\n",
      "l05.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l05.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l06.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l06.bias             | grad.mean=+nan | grad.std=nan\n",
      "l06.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l06.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l07.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l07.bias             | grad.mean=+nan | grad.std=nan\n",
      "l07.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l07.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l08.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l08.bias             | grad.mean=+nan | grad.std=nan\n",
      "l08.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l08.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l09.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l09.bias             | grad.mean=+nan | grad.std=nan\n",
      "l09.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l09.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l10.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l10.bias             | grad.mean=+nan | grad.std=nan\n",
      "l10.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l10.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l11.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l11.bias             | grad.mean=+nan | grad.std=nan\n",
      "l11.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l11.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l12.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l12.bias             | grad.mean=+nan | grad.std=nan\n",
      "l12.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l12.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "lfi.gamma            | grad.mean=+nan | grad.std=nan\n",
      "lfi.bias             | grad.mean=+nan | grad.std=nan\n",
      "lfi.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "lfi.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "####################\n",
      "Parámetro: l01.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l01.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l01.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[1.1537],\n",
      "        [0.6830],\n",
      "        [1.0407],\n",
      "        [0.9750],\n",
      "        [1.0204]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l01.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 3])\n",
      "Dim parámetro: tensor([[ 0.6559,  0.6975, -0.6437],\n",
      "        [ 0.5295,  0.1893,  0.3877],\n",
      "        [ 0.5463, -0.5104,  0.7239],\n",
      "        [-0.0981, -0.4058,  0.8811],\n",
      "        [ 0.8861, -0.3403, -0.3746]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l02.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l02.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l02.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.4389],\n",
      "        [13.5673],\n",
      "        [14.6712],\n",
      "        [13.7905],\n",
      "        [14.6239]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l02.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[ 0.6871,  0.9561,  0.1057,  ..., -0.2759,  0.2907,  0.3948],\n",
      "        [ 0.6835,  0.0776, -0.5742,  ...,  0.0057, -0.1256, -0.0636],\n",
      "        [ 0.6685, -0.5220,  0.6241,  ...,  0.9472, -0.7029,  0.2562],\n",
      "        [ 0.8483, -0.5695, -0.1631,  ...,  0.8482,  0.4483, -0.7179],\n",
      "        [ 0.5217,  0.5519,  0.9914,  ...,  0.8378,  0.8397,  0.4180]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l03.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l03.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l03.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.2694],\n",
      "        [14.0656],\n",
      "        [13.8109],\n",
      "        [14.1541],\n",
      "        [13.5901]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l03.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.4708, -0.6973, -0.3287,  ...,  0.1222, -0.6993, -0.5033],\n",
      "        [-0.1913,  0.2383,  0.7957,  ..., -0.8634,  0.0478,  0.4068],\n",
      "        [-0.6836, -0.3484,  0.9402,  ...,  0.1655,  0.7878, -0.1465],\n",
      "        [-0.6792,  0.5930, -0.1439,  ..., -0.1030, -0.7344,  0.3552],\n",
      "        [ 0.7082,  0.0383, -0.8814,  ..., -0.1800,  0.1463, -0.2389]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l04.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l04.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l04.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[13.7767],\n",
      "        [14.5479],\n",
      "        [14.0371],\n",
      "        [13.9903],\n",
      "        [14.5850]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l04.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.2132, -0.3289, -0.6671,  ...,  0.2703,  0.6832, -0.6609],\n",
      "        [ 0.4969,  0.1876, -0.5401,  ..., -0.7251,  0.1291, -0.7737],\n",
      "        [-0.4815, -0.6621, -0.0262,  ...,  0.2020, -0.1731, -0.7846],\n",
      "        [ 0.6913, -0.8229,  0.5706,  ..., -0.8525,  0.1842,  0.3531],\n",
      "        [ 0.8193,  0.8788, -0.7583,  ..., -0.2512, -0.4285, -0.9829]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l05.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l05.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l05.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.3453],\n",
      "        [14.2198],\n",
      "        [14.0731],\n",
      "        [14.3287],\n",
      "        [14.4587]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l05.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.5535, -0.6388, -0.9791,  ...,  0.2012, -0.2136,  0.1824],\n",
      "        [ 0.7800,  0.4213,  0.5066,  ..., -0.8905,  0.9592, -0.9390],\n",
      "        [ 0.0546, -0.5997,  0.2778,  ...,  0.7730, -0.0212,  0.8454],\n",
      "        [-0.0962, -0.1040, -0.9505,  ..., -0.0395, -0.7370, -0.4685],\n",
      "        [-0.2736, -0.0892, -0.9661,  ..., -0.7093, -0.0057, -0.6542]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l06.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l06.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l06.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.2913],\n",
      "        [13.9985],\n",
      "        [14.3805],\n",
      "        [14.1601],\n",
      "        [14.3357]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l06.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[ 0.6443,  0.6010, -0.9351,  ...,  0.8518,  0.0367, -0.1698],\n",
      "        [ 0.9864, -0.2879,  0.6890,  ...,  0.2745,  0.5200,  0.7622],\n",
      "        [ 0.8547,  0.9969,  0.9620,  ..., -0.7654, -0.7354, -0.8518],\n",
      "        [-0.2772,  0.3475, -0.0488,  ...,  0.5700, -0.4006,  0.1645],\n",
      "        [-0.6994, -0.4046,  0.8345,  ..., -0.9519, -0.2276, -0.6924]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l07.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l07.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l07.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.4121],\n",
      "        [13.9767],\n",
      "        [13.8591],\n",
      "        [14.0035],\n",
      "        [13.8459]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l07.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.5673,  0.4738,  0.3427,  ..., -0.2057, -0.9854,  0.0426],\n",
      "        [ 0.5489, -0.0853, -0.7776,  ..., -0.3139,  0.9274,  0.3501],\n",
      "        [-0.9789, -0.3269, -0.3954,  ...,  0.5816, -0.6826, -0.3930],\n",
      "        [-0.8702, -0.3840, -0.8146,  ...,  0.6614, -0.9949,  0.0586],\n",
      "        [-0.0821, -0.9500,  0.9391,  ..., -0.6365, -0.0804, -0.0154]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l08.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l08.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l08.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.1317],\n",
      "        [14.3957],\n",
      "        [14.2970],\n",
      "        [13.8194],\n",
      "        [13.9807]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l08.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.0684,  0.5977,  0.8942,  ...,  0.3460, -0.9562, -0.2035],\n",
      "        [ 0.6116, -0.7567,  0.9228,  ..., -0.3851,  0.2864, -0.7013],\n",
      "        [-0.0917, -0.5360, -0.9290,  ..., -0.6399, -0.2008,  0.2293],\n",
      "        [ 0.5559,  0.8348, -0.4002,  ...,  0.2520,  0.5939, -0.2458],\n",
      "        [ 0.9462,  0.7383,  0.4689,  ...,  0.6315,  0.5102,  0.6758]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l09.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l09.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l09.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.0965],\n",
      "        [14.1900],\n",
      "        [13.9275],\n",
      "        [14.1550],\n",
      "        [13.7451]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l09.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.9229,  0.6505,  0.1473,  ...,  0.5044,  0.6742, -0.7342],\n",
      "        [-0.4551, -0.7742,  0.8113,  ...,  0.5573,  0.8533,  0.7311],\n",
      "        [ 0.8350, -0.6763, -0.8058,  ...,  0.6354,  0.5372,  0.4186],\n",
      "        [ 0.8052, -0.0519, -0.2502,  ...,  0.3148,  0.3226, -0.4012],\n",
      "        [-0.7149,  0.1576,  0.1890,  ...,  0.9033,  0.8529, -0.1539]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l10.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l10.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l10.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.1593],\n",
      "        [13.8843],\n",
      "        [14.2610],\n",
      "        [14.6419],\n",
      "        [14.1405]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l10.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.2141,  0.0182,  0.2382,  ...,  0.7561,  0.6285, -0.8943],\n",
      "        [ 0.8400,  0.2460,  0.1895,  ..., -0.8183, -0.5697, -0.2678],\n",
      "        [ 0.5407, -0.6177, -0.8056,  ..., -0.2077,  0.1404, -0.5982],\n",
      "        [-0.5825,  0.4528,  0.3377,  ..., -0.9031, -0.5521, -0.9283],\n",
      "        [ 0.4996,  0.9934,  0.3813,  ..., -0.6539, -0.0783, -0.9421]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l11.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l11.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l11.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[13.9311],\n",
      "        [14.0192],\n",
      "        [14.3504],\n",
      "        [13.7961],\n",
      "        [14.0364]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l11.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-1.0252e-01, -8.4855e-01, -8.6159e-01,  ..., -1.6484e-01,\n",
      "          3.4293e-01,  7.5845e-01],\n",
      "        [ 9.2328e-04,  7.6889e-01,  1.3378e-01,  ...,  8.0115e-01,\n",
      "         -1.5561e-01,  4.2724e-02],\n",
      "        [-1.4773e-01,  8.4083e-01,  9.9066e-01,  ..., -7.6634e-01,\n",
      "         -4.2618e-01, -5.1494e-01],\n",
      "        [ 2.4558e-01, -2.0621e-01,  9.5098e-01,  ..., -6.2394e-01,\n",
      "         -5.7332e-01, -7.3193e-01],\n",
      "        [ 8.7902e-01, -6.9896e-01, -8.2742e-01,  ..., -7.1601e-01,\n",
      "         -4.5050e-01, -3.9147e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l12.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l12.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l12.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.3955],\n",
      "        [14.7006],\n",
      "        [14.1305],\n",
      "        [14.0103],\n",
      "        [14.1039]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l12.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[ 0.3990, -0.1525, -0.8500,  ...,  0.1287,  0.4216, -0.8424],\n",
      "        [-0.7158,  0.8529, -0.2187,  ..., -0.9610,  0.3691, -0.6698],\n",
      "        [ 0.2980,  0.7911,  0.4784,  ..., -0.4754, -0.3311, -0.8379],\n",
      "        [-0.9810,  0.8042,  0.6447,  ..., -0.3274,  0.4195,  0.7202],\n",
      "        [-0.0192, -0.9565, -0.9786,  ..., -0.8798,  0.8914,  0.0606]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: lfi.gamma \n",
      "Dim parámetro: torch.Size([3])\n",
      "Dim parámetro: tensor([1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: lfi.bias  \n",
      "Dim parámetro: torch.Size([3])\n",
      "Dim parámetro: tensor([0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: lfi.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([3, 1])\n",
      "Dim parámetro: tensor([[14.4558],\n",
      "        [13.9026],\n",
      "        [14.4056]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: lfi.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([3, 600])\n",
      "Dim parámetro: tensor([[-0.5025, -0.5699,  0.5540,  ...,  0.5311,  0.1282,  0.2799],\n",
      "        [-0.3356, -0.5155,  0.3135,  ..., -0.9594, -0.1095, -0.3112],\n",
      "        [ 0.4784,  0.5623,  0.0107,  ..., -0.8606, -0.8373, -0.7438]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "Capa 00: shape=(231, 600), mean=-0.0096, std=0.9487\n",
      "Capa 01: shape=(231, 600), mean=-0.0045, std=0.9715\n",
      "Capa 02: shape=(231, 600), mean=-0.0426, std=0.9705\n",
      "Capa 03: shape=(231, 600), mean=-0.0481, std=0.9674\n",
      "Capa 04: shape=(231, 600), mean=+0.0089, std=0.9705\n",
      "Capa 05: shape=(231, 600), mean=-0.0314, std=0.9709\n",
      "Capa 06: shape=(231, 600), mean=+0.0303, std=0.9709\n",
      "Capa 07: shape=(231, 600), mean=-0.0180, std=0.9704\n",
      "Capa 08: shape=(231, 600), mean=+0.0077, std=13.7389\n",
      "Capa 09: shape=(231, 600), mean=-6.0112, std=193.2561\n",
      "Capa 10: shape=(231, 600), mean=-37.8212, std=2721.2961\n",
      "Capa 11: shape=(231, 600), mean=-549.3846, std=39010.4531\n",
      "Capa 12: shape=(231, 3), mean=+100103.5234, std=519539.3125\n",
      "[WARN] grad no finito en l01.gamma\n",
      "Capa 00: shape=(231, 600), mean=-0.0095, std=0.9446\n",
      "Capa 01: shape=(231, 600), mean=-0.0058, std=0.9707\n",
      "Capa 02: shape=(231, 600), mean=-0.0428, std=0.9707\n",
      "Capa 03: shape=(231, 600), mean=-0.0494, std=0.9670\n",
      "Capa 04: shape=(231, 600), mean=+0.0104, std=0.9701\n",
      "Capa 05: shape=(231, 600), mean=-0.0285, std=0.9710\n",
      "Capa 06: shape=(231, 600), mean=+0.0338, std=0.9705\n",
      "Capa 07: shape=(231, 600), mean=-0.0185, std=0.9705\n",
      "Capa 08: shape=(231, 600), mean=+0.0014, std=13.6999\n",
      "Capa 09: shape=(231, 600), mean=-5.5599, std=193.3648\n",
      "Capa 10: shape=(231, 600), mean=-38.5798, std=2720.2146\n",
      "Capa 11: shape=(231, 600), mean=-348.4278, std=38957.3320\n",
      "Capa 12: shape=(231, 3), mean=+91290.3281, std=509902.7812\n",
      "Epoch:    2 | Loss training:        inf | NS_Loss: 1.8045e+28 | U_Loss: 7.1637e+12 | V_Loss: 1.0511e+13 | P_Loss: 2.6322e+10 | learning rate: 1.0000e-03 | \n",
      "\n",
      "=== GRADIENTS ===\n",
      "l01.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l01.bias             | grad.mean=+nan | grad.std=nan\n",
      "l01.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l01.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l02.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l02.bias             | grad.mean=+nan | grad.std=nan\n",
      "l02.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l02.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l03.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l03.bias             | grad.mean=+nan | grad.std=nan\n",
      "l03.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l03.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l04.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l04.bias             | grad.mean=+nan | grad.std=nan\n",
      "l04.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l04.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l05.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l05.bias             | grad.mean=+nan | grad.std=nan\n",
      "l05.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l05.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l06.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l06.bias             | grad.mean=+nan | grad.std=nan\n",
      "l06.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l06.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l07.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l07.bias             | grad.mean=+nan | grad.std=nan\n",
      "l07.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l07.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l08.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l08.bias             | grad.mean=+nan | grad.std=nan\n",
      "l08.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l08.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l09.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l09.bias             | grad.mean=+nan | grad.std=nan\n",
      "l09.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l09.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l10.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l10.bias             | grad.mean=+nan | grad.std=nan\n",
      "l10.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l10.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l11.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l11.bias             | grad.mean=+nan | grad.std=nan\n",
      "l11.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l11.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l12.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l12.bias             | grad.mean=+nan | grad.std=nan\n",
      "l12.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l12.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "lfi.gamma            | grad.mean=+nan | grad.std=nan\n",
      "lfi.bias             | grad.mean=+nan | grad.std=nan\n",
      "lfi.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "lfi.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "####################\n",
      "Parámetro: l01.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l01.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l01.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[1.1537],\n",
      "        [0.6830],\n",
      "        [1.0407],\n",
      "        [0.9750],\n",
      "        [1.0204]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l01.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 3])\n",
      "Dim parámetro: tensor([[ 0.6559,  0.6975, -0.6437],\n",
      "        [ 0.5295,  0.1893,  0.3877],\n",
      "        [ 0.5463, -0.5104,  0.7239],\n",
      "        [-0.0981, -0.4058,  0.8811],\n",
      "        [ 0.8861, -0.3403, -0.3746]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l02.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l02.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l02.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.4389],\n",
      "        [13.5673],\n",
      "        [14.6712],\n",
      "        [13.7905],\n",
      "        [14.6239]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l02.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[ 0.6871,  0.9561,  0.1057,  ..., -0.2759,  0.2907,  0.3948],\n",
      "        [ 0.6835,  0.0776, -0.5742,  ...,  0.0057, -0.1256, -0.0636],\n",
      "        [ 0.6685, -0.5220,  0.6241,  ...,  0.9472, -0.7029,  0.2562],\n",
      "        [ 0.8483, -0.5695, -0.1631,  ...,  0.8482,  0.4483, -0.7179],\n",
      "        [ 0.5217,  0.5519,  0.9914,  ...,  0.8378,  0.8397,  0.4180]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l03.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l03.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l03.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.2694],\n",
      "        [14.0656],\n",
      "        [13.8109],\n",
      "        [14.1541],\n",
      "        [13.5901]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l03.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.4708, -0.6973, -0.3287,  ...,  0.1222, -0.6993, -0.5033],\n",
      "        [-0.1913,  0.2383,  0.7957,  ..., -0.8634,  0.0478,  0.4068],\n",
      "        [-0.6836, -0.3484,  0.9402,  ...,  0.1655,  0.7878, -0.1465],\n",
      "        [-0.6792,  0.5930, -0.1439,  ..., -0.1030, -0.7344,  0.3552],\n",
      "        [ 0.7082,  0.0383, -0.8814,  ..., -0.1800,  0.1463, -0.2389]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l04.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l04.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l04.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[13.7767],\n",
      "        [14.5479],\n",
      "        [14.0371],\n",
      "        [13.9903],\n",
      "        [14.5850]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l04.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.2132, -0.3289, -0.6671,  ...,  0.2703,  0.6832, -0.6609],\n",
      "        [ 0.4969,  0.1876, -0.5401,  ..., -0.7251,  0.1291, -0.7737],\n",
      "        [-0.4815, -0.6621, -0.0262,  ...,  0.2020, -0.1731, -0.7846],\n",
      "        [ 0.6913, -0.8229,  0.5706,  ..., -0.8525,  0.1842,  0.3531],\n",
      "        [ 0.8193,  0.8788, -0.7583,  ..., -0.2512, -0.4285, -0.9829]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l05.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l05.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l05.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.3453],\n",
      "        [14.2198],\n",
      "        [14.0731],\n",
      "        [14.3287],\n",
      "        [14.4587]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l05.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.5535, -0.6388, -0.9791,  ...,  0.2012, -0.2136,  0.1824],\n",
      "        [ 0.7800,  0.4213,  0.5066,  ..., -0.8905,  0.9592, -0.9390],\n",
      "        [ 0.0546, -0.5997,  0.2778,  ...,  0.7730, -0.0212,  0.8454],\n",
      "        [-0.0962, -0.1040, -0.9505,  ..., -0.0395, -0.7370, -0.4685],\n",
      "        [-0.2736, -0.0892, -0.9661,  ..., -0.7093, -0.0057, -0.6542]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l06.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l06.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l06.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.2913],\n",
      "        [13.9985],\n",
      "        [14.3805],\n",
      "        [14.1601],\n",
      "        [14.3357]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l06.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[ 0.6443,  0.6010, -0.9351,  ...,  0.8518,  0.0367, -0.1698],\n",
      "        [ 0.9864, -0.2879,  0.6890,  ...,  0.2745,  0.5200,  0.7622],\n",
      "        [ 0.8547,  0.9969,  0.9620,  ..., -0.7654, -0.7354, -0.8518],\n",
      "        [-0.2772,  0.3475, -0.0488,  ...,  0.5700, -0.4006,  0.1645],\n",
      "        [-0.6994, -0.4046,  0.8345,  ..., -0.9519, -0.2276, -0.6924]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l07.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l07.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l07.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.4121],\n",
      "        [13.9767],\n",
      "        [13.8591],\n",
      "        [14.0035],\n",
      "        [13.8459]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l07.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.5673,  0.4738,  0.3427,  ..., -0.2057, -0.9854,  0.0426],\n",
      "        [ 0.5489, -0.0853, -0.7776,  ..., -0.3139,  0.9274,  0.3501],\n",
      "        [-0.9789, -0.3269, -0.3954,  ...,  0.5816, -0.6826, -0.3930],\n",
      "        [-0.8702, -0.3840, -0.8146,  ...,  0.6614, -0.9949,  0.0586],\n",
      "        [-0.0821, -0.9500,  0.9391,  ..., -0.6365, -0.0804, -0.0154]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l08.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l08.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l08.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.1317],\n",
      "        [14.3957],\n",
      "        [14.2970],\n",
      "        [13.8194],\n",
      "        [13.9807]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l08.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.0684,  0.5977,  0.8942,  ...,  0.3460, -0.9562, -0.2035],\n",
      "        [ 0.6116, -0.7567,  0.9228,  ..., -0.3851,  0.2864, -0.7013],\n",
      "        [-0.0917, -0.5360, -0.9290,  ..., -0.6399, -0.2008,  0.2293],\n",
      "        [ 0.5559,  0.8348, -0.4002,  ...,  0.2520,  0.5939, -0.2458],\n",
      "        [ 0.9462,  0.7383,  0.4689,  ...,  0.6315,  0.5102,  0.6758]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l09.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l09.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l09.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.0965],\n",
      "        [14.1900],\n",
      "        [13.9275],\n",
      "        [14.1550],\n",
      "        [13.7451]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l09.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.9229,  0.6505,  0.1473,  ...,  0.5044,  0.6742, -0.7342],\n",
      "        [-0.4551, -0.7742,  0.8113,  ...,  0.5573,  0.8533,  0.7311],\n",
      "        [ 0.8350, -0.6763, -0.8058,  ...,  0.6354,  0.5372,  0.4186],\n",
      "        [ 0.8052, -0.0519, -0.2502,  ...,  0.3148,  0.3226, -0.4012],\n",
      "        [-0.7149,  0.1576,  0.1890,  ...,  0.9033,  0.8529, -0.1539]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l10.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l10.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l10.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.1593],\n",
      "        [13.8843],\n",
      "        [14.2610],\n",
      "        [14.6419],\n",
      "        [14.1405]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l10.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.2141,  0.0182,  0.2382,  ...,  0.7561,  0.6285, -0.8943],\n",
      "        [ 0.8400,  0.2460,  0.1895,  ..., -0.8183, -0.5697, -0.2678],\n",
      "        [ 0.5407, -0.6177, -0.8056,  ..., -0.2077,  0.1404, -0.5982],\n",
      "        [-0.5825,  0.4528,  0.3377,  ..., -0.9031, -0.5521, -0.9283],\n",
      "        [ 0.4996,  0.9934,  0.3813,  ..., -0.6539, -0.0783, -0.9421]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l11.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l11.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l11.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[13.9311],\n",
      "        [14.0192],\n",
      "        [14.3504],\n",
      "        [13.7961],\n",
      "        [14.0364]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l11.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-1.0252e-01, -8.4855e-01, -8.6159e-01,  ..., -1.6484e-01,\n",
      "          3.4293e-01,  7.5845e-01],\n",
      "        [ 9.2328e-04,  7.6889e-01,  1.3378e-01,  ...,  8.0115e-01,\n",
      "         -1.5561e-01,  4.2724e-02],\n",
      "        [-1.4773e-01,  8.4083e-01,  9.9066e-01,  ..., -7.6634e-01,\n",
      "         -4.2618e-01, -5.1494e-01],\n",
      "        [ 2.4558e-01, -2.0621e-01,  9.5098e-01,  ..., -6.2394e-01,\n",
      "         -5.7332e-01, -7.3193e-01],\n",
      "        [ 8.7902e-01, -6.9896e-01, -8.2742e-01,  ..., -7.1601e-01,\n",
      "         -4.5050e-01, -3.9147e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l12.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l12.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l12.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.3955],\n",
      "        [14.7006],\n",
      "        [14.1305],\n",
      "        [14.0103],\n",
      "        [14.1039]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l12.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[ 0.3990, -0.1525, -0.8500,  ...,  0.1287,  0.4216, -0.8424],\n",
      "        [-0.7158,  0.8529, -0.2187,  ..., -0.9610,  0.3691, -0.6698],\n",
      "        [ 0.2980,  0.7911,  0.4784,  ..., -0.4754, -0.3311, -0.8379],\n",
      "        [-0.9810,  0.8042,  0.6447,  ..., -0.3274,  0.4195,  0.7202],\n",
      "        [-0.0192, -0.9565, -0.9786,  ..., -0.8798,  0.8914,  0.0606]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: lfi.gamma \n",
      "Dim parámetro: torch.Size([3])\n",
      "Dim parámetro: tensor([1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: lfi.bias  \n",
      "Dim parámetro: torch.Size([3])\n",
      "Dim parámetro: tensor([0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: lfi.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([3, 1])\n",
      "Dim parámetro: tensor([[14.4558],\n",
      "        [13.9026],\n",
      "        [14.4056]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: lfi.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([3, 600])\n",
      "Dim parámetro: tensor([[-0.5025, -0.5699,  0.5540,  ...,  0.5311,  0.1282,  0.2799],\n",
      "        [-0.3356, -0.5155,  0.3135,  ..., -0.9594, -0.1095, -0.3112],\n",
      "        [ 0.4784,  0.5623,  0.0107,  ..., -0.8606, -0.8373, -0.7438]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "Capa 00: shape=(231, 600), mean=-0.0093, std=0.9358\n",
      "Capa 01: shape=(231, 600), mean=-0.0058, std=0.9687\n",
      "Capa 02: shape=(231, 600), mean=-0.0420, std=0.9703\n",
      "Capa 03: shape=(231, 600), mean=-0.0470, std=0.9673\n",
      "Capa 04: shape=(231, 600), mean=+0.0120, std=0.9702\n",
      "Capa 05: shape=(231, 600), mean=-0.0333, std=0.9710\n",
      "Capa 06: shape=(231, 600), mean=+0.0336, std=0.9708\n",
      "Capa 07: shape=(231, 600), mean=-0.0193, std=0.9708\n",
      "Capa 08: shape=(231, 600), mean=-0.0079, std=13.7561\n",
      "Capa 09: shape=(231, 600), mean=-6.0048, std=193.4417\n",
      "Capa 10: shape=(231, 600), mean=-30.1611, std=2720.7556\n",
      "Capa 11: shape=(231, 600), mean=-225.2999, std=38883.8555\n",
      "Capa 12: shape=(231, 3), mean=+73898.9688, std=558823.8750\n",
      "[WARN] grad no finito en l01.gamma\n",
      "Capa 00: shape=(231, 600), mean=-0.0094, std=0.9410\n",
      "Capa 01: shape=(231, 600), mean=-0.0058, std=0.9694\n",
      "Capa 02: shape=(231, 600), mean=-0.0434, std=0.9703\n",
      "Capa 03: shape=(231, 600), mean=-0.0485, std=0.9674\n",
      "Capa 04: shape=(231, 600), mean=+0.0093, std=0.9702\n",
      "Capa 05: shape=(231, 600), mean=-0.0301, std=0.9712\n",
      "Capa 06: shape=(231, 600), mean=+0.0274, std=0.9708\n",
      "Capa 07: shape=(231, 600), mean=-0.0169, std=0.9708\n",
      "Capa 08: shape=(231, 600), mean=+0.0234, std=13.7464\n",
      "Capa 09: shape=(231, 600), mean=-6.1308, std=193.4505\n",
      "Capa 10: shape=(231, 600), mean=-35.6881, std=2724.2161\n",
      "Capa 11: shape=(231, 600), mean=-139.4013, std=39031.8555\n",
      "Capa 12: shape=(231, 3), mean=+79840.4844, std=508197.2812\n",
      "Epoch:    3 | Loss training:        inf | NS_Loss: 1.8488e+28 | U_Loss: 8.7433e+12 | V_Loss: 1.1412e+13 | P_Loss: 2.6090e+10 | learning rate: 1.0000e-03 | \n",
      "\n",
      "=== GRADIENTS ===\n",
      "l01.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l01.bias             | grad.mean=+nan | grad.std=nan\n",
      "l01.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l01.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l02.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l02.bias             | grad.mean=+nan | grad.std=nan\n",
      "l02.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l02.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l03.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l03.bias             | grad.mean=+nan | grad.std=nan\n",
      "l03.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l03.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l04.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l04.bias             | grad.mean=+nan | grad.std=nan\n",
      "l04.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l04.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l05.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l05.bias             | grad.mean=+nan | grad.std=nan\n",
      "l05.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l05.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l06.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l06.bias             | grad.mean=+nan | grad.std=nan\n",
      "l06.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l06.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l07.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l07.bias             | grad.mean=+nan | grad.std=nan\n",
      "l07.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l07.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l08.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l08.bias             | grad.mean=+nan | grad.std=nan\n",
      "l08.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l08.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l09.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l09.bias             | grad.mean=+nan | grad.std=nan\n",
      "l09.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l09.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l10.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l10.bias             | grad.mean=+nan | grad.std=nan\n",
      "l10.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l10.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l11.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l11.bias             | grad.mean=+nan | grad.std=nan\n",
      "l11.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l11.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "l12.gamma            | grad.mean=+nan | grad.std=nan\n",
      "l12.bias             | grad.mean=+nan | grad.std=nan\n",
      "l12.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "l12.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "lfi.gamma            | grad.mean=+nan | grad.std=nan\n",
      "lfi.bias             | grad.mean=+nan | grad.std=nan\n",
      "lfi.w.parametrizations.weight.original0 | grad.mean=+nan | grad.std=nan\n",
      "lfi.w.parametrizations.weight.original1 | grad.mean=+nan | grad.std=nan\n",
      "####################\n",
      "Parámetro: l01.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l01.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l01.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[1.1537],\n",
      "        [0.6830],\n",
      "        [1.0407],\n",
      "        [0.9750],\n",
      "        [1.0204]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l01.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 3])\n",
      "Dim parámetro: tensor([[ 0.6559,  0.6975, -0.6437],\n",
      "        [ 0.5295,  0.1893,  0.3877],\n",
      "        [ 0.5463, -0.5104,  0.7239],\n",
      "        [-0.0981, -0.4058,  0.8811],\n",
      "        [ 0.8861, -0.3403, -0.3746]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l02.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l02.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l02.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.4389],\n",
      "        [13.5673],\n",
      "        [14.6712],\n",
      "        [13.7905],\n",
      "        [14.6239]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l02.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[ 0.6871,  0.9561,  0.1057,  ..., -0.2759,  0.2907,  0.3948],\n",
      "        [ 0.6835,  0.0776, -0.5742,  ...,  0.0057, -0.1256, -0.0636],\n",
      "        [ 0.6685, -0.5220,  0.6241,  ...,  0.9472, -0.7029,  0.2562],\n",
      "        [ 0.8483, -0.5695, -0.1631,  ...,  0.8482,  0.4483, -0.7179],\n",
      "        [ 0.5217,  0.5519,  0.9914,  ...,  0.8378,  0.8397,  0.4180]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l03.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l03.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l03.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.2694],\n",
      "        [14.0656],\n",
      "        [13.8109],\n",
      "        [14.1541],\n",
      "        [13.5901]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l03.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.4708, -0.6973, -0.3287,  ...,  0.1222, -0.6993, -0.5033],\n",
      "        [-0.1913,  0.2383,  0.7957,  ..., -0.8634,  0.0478,  0.4068],\n",
      "        [-0.6836, -0.3484,  0.9402,  ...,  0.1655,  0.7878, -0.1465],\n",
      "        [-0.6792,  0.5930, -0.1439,  ..., -0.1030, -0.7344,  0.3552],\n",
      "        [ 0.7082,  0.0383, -0.8814,  ..., -0.1800,  0.1463, -0.2389]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l04.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l04.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l04.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[13.7767],\n",
      "        [14.5479],\n",
      "        [14.0371],\n",
      "        [13.9903],\n",
      "        [14.5850]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l04.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.2132, -0.3289, -0.6671,  ...,  0.2703,  0.6832, -0.6609],\n",
      "        [ 0.4969,  0.1876, -0.5401,  ..., -0.7251,  0.1291, -0.7737],\n",
      "        [-0.4815, -0.6621, -0.0262,  ...,  0.2020, -0.1731, -0.7846],\n",
      "        [ 0.6913, -0.8229,  0.5706,  ..., -0.8525,  0.1842,  0.3531],\n",
      "        [ 0.8193,  0.8788, -0.7583,  ..., -0.2512, -0.4285, -0.9829]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l05.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l05.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l05.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.3453],\n",
      "        [14.2198],\n",
      "        [14.0731],\n",
      "        [14.3287],\n",
      "        [14.4587]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l05.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.5535, -0.6388, -0.9791,  ...,  0.2012, -0.2136,  0.1824],\n",
      "        [ 0.7800,  0.4213,  0.5066,  ..., -0.8905,  0.9592, -0.9390],\n",
      "        [ 0.0546, -0.5997,  0.2778,  ...,  0.7730, -0.0212,  0.8454],\n",
      "        [-0.0962, -0.1040, -0.9505,  ..., -0.0395, -0.7370, -0.4685],\n",
      "        [-0.2736, -0.0892, -0.9661,  ..., -0.7093, -0.0057, -0.6542]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l06.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l06.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l06.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.2913],\n",
      "        [13.9985],\n",
      "        [14.3805],\n",
      "        [14.1601],\n",
      "        [14.3357]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l06.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[ 0.6443,  0.6010, -0.9351,  ...,  0.8518,  0.0367, -0.1698],\n",
      "        [ 0.9864, -0.2879,  0.6890,  ...,  0.2745,  0.5200,  0.7622],\n",
      "        [ 0.8547,  0.9969,  0.9620,  ..., -0.7654, -0.7354, -0.8518],\n",
      "        [-0.2772,  0.3475, -0.0488,  ...,  0.5700, -0.4006,  0.1645],\n",
      "        [-0.6994, -0.4046,  0.8345,  ..., -0.9519, -0.2276, -0.6924]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l07.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l07.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l07.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.4121],\n",
      "        [13.9767],\n",
      "        [13.8591],\n",
      "        [14.0035],\n",
      "        [13.8459]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l07.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.5673,  0.4738,  0.3427,  ..., -0.2057, -0.9854,  0.0426],\n",
      "        [ 0.5489, -0.0853, -0.7776,  ..., -0.3139,  0.9274,  0.3501],\n",
      "        [-0.9789, -0.3269, -0.3954,  ...,  0.5816, -0.6826, -0.3930],\n",
      "        [-0.8702, -0.3840, -0.8146,  ...,  0.6614, -0.9949,  0.0586],\n",
      "        [-0.0821, -0.9500,  0.9391,  ..., -0.6365, -0.0804, -0.0154]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l08.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l08.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l08.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.1317],\n",
      "        [14.3957],\n",
      "        [14.2970],\n",
      "        [13.8194],\n",
      "        [13.9807]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l08.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.0684,  0.5977,  0.8942,  ...,  0.3460, -0.9562, -0.2035],\n",
      "        [ 0.6116, -0.7567,  0.9228,  ..., -0.3851,  0.2864, -0.7013],\n",
      "        [-0.0917, -0.5360, -0.9290,  ..., -0.6399, -0.2008,  0.2293],\n",
      "        [ 0.5559,  0.8348, -0.4002,  ...,  0.2520,  0.5939, -0.2458],\n",
      "        [ 0.9462,  0.7383,  0.4689,  ...,  0.6315,  0.5102,  0.6758]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l09.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l09.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l09.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.0965],\n",
      "        [14.1900],\n",
      "        [13.9275],\n",
      "        [14.1550],\n",
      "        [13.7451]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l09.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.9229,  0.6505,  0.1473,  ...,  0.5044,  0.6742, -0.7342],\n",
      "        [-0.4551, -0.7742,  0.8113,  ...,  0.5573,  0.8533,  0.7311],\n",
      "        [ 0.8350, -0.6763, -0.8058,  ...,  0.6354,  0.5372,  0.4186],\n",
      "        [ 0.8052, -0.0519, -0.2502,  ...,  0.3148,  0.3226, -0.4012],\n",
      "        [-0.7149,  0.1576,  0.1890,  ...,  0.9033,  0.8529, -0.1539]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l10.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l10.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l10.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.1593],\n",
      "        [13.8843],\n",
      "        [14.2610],\n",
      "        [14.6419],\n",
      "        [14.1405]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l10.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.2141,  0.0182,  0.2382,  ...,  0.7561,  0.6285, -0.8943],\n",
      "        [ 0.8400,  0.2460,  0.1895,  ..., -0.8183, -0.5697, -0.2678],\n",
      "        [ 0.5407, -0.6177, -0.8056,  ..., -0.2077,  0.1404, -0.5982],\n",
      "        [-0.5825,  0.4528,  0.3377,  ..., -0.9031, -0.5521, -0.9283],\n",
      "        [ 0.4996,  0.9934,  0.3813,  ..., -0.6539, -0.0783, -0.9421]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l11.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l11.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l11.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[13.9311],\n",
      "        [14.0192],\n",
      "        [14.3504],\n",
      "        [13.7961],\n",
      "        [14.0364]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l11.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-1.0252e-01, -8.4855e-01, -8.6159e-01,  ..., -1.6484e-01,\n",
      "          3.4293e-01,  7.5845e-01],\n",
      "        [ 9.2328e-04,  7.6889e-01,  1.3378e-01,  ...,  8.0115e-01,\n",
      "         -1.5561e-01,  4.2724e-02],\n",
      "        [-1.4773e-01,  8.4083e-01,  9.9066e-01,  ..., -7.6634e-01,\n",
      "         -4.2618e-01, -5.1494e-01],\n",
      "        [ 2.4558e-01, -2.0621e-01,  9.5098e-01,  ..., -6.2394e-01,\n",
      "         -5.7332e-01, -7.3193e-01],\n",
      "        [ 8.7902e-01, -6.9896e-01, -8.2742e-01,  ..., -7.1601e-01,\n",
      "         -4.5050e-01, -3.9147e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l12.gamma \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l12.bias  \n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l12.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([600, 1])\n",
      "Dim parámetro: tensor([[14.3955],\n",
      "        [14.7006],\n",
      "        [14.1305],\n",
      "        [14.0103],\n",
      "        [14.1039]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: l12.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[ 0.3990, -0.1525, -0.8500,  ...,  0.1287,  0.4216, -0.8424],\n",
      "        [-0.7158,  0.8529, -0.2187,  ..., -0.9610,  0.3691, -0.6698],\n",
      "        [ 0.2980,  0.7911,  0.4784,  ..., -0.4754, -0.3311, -0.8379],\n",
      "        [-0.9810,  0.8042,  0.6447,  ..., -0.3274,  0.4195,  0.7202],\n",
      "        [-0.0192, -0.9565, -0.9786,  ..., -0.8798,  0.8914,  0.0606]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: lfi.gamma \n",
      "Dim parámetro: torch.Size([3])\n",
      "Dim parámetro: tensor([1., 1., 1.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: lfi.bias  \n",
      "Dim parámetro: torch.Size([3])\n",
      "Dim parámetro: tensor([0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: lfi.w.parametrizations.weight.original0\n",
      "Dim parámetro: torch.Size([3, 1])\n",
      "Dim parámetro: tensor([[14.4558],\n",
      "        [13.9026],\n",
      "        [14.4056]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: lfi.w.parametrizations.weight.original1\n",
      "Dim parámetro: torch.Size([3, 600])\n",
      "Dim parámetro: tensor([[-0.5025, -0.5699,  0.5540,  ...,  0.5311,  0.1282,  0.2799],\n",
      "        [-0.3356, -0.5155,  0.3135,  ..., -0.9594, -0.1095, -0.3112],\n",
      "        [ 0.4784,  0.5623,  0.0107,  ..., -0.8606, -0.8373, -0.7438]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     52\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m20s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | grad=None\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     54\u001b[39m info_model(model)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Proyectos/pinns_bruselas/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1473\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1471\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1472\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1473\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1474\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1475\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_shell_parent_ident\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1476\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1477\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1478\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Proyectos/pinns_bruselas/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1518\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1516\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1517\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1518\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1519\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1520\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Obten la mejor pérdida \n",
    "major_loss_validation = float('inf')\n",
    "\n",
    "# Keep results for plotting\n",
    "train_loss_results = []\n",
    "NS_loss_results = []\n",
    "P_loss_results = []\n",
    "U_loss_results = []\n",
    "V_loss_results = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Inicializamos registros de las funciones de perdida\n",
    "    epoch_loss_avg = RunningMean()\n",
    "    epoch_NS_loss_avg = RunningMean()\n",
    "    epoch_P_loss_avg = RunningMean()\n",
    "    epoch_U_loss_avg = RunningMean()\n",
    "    epoch_V_loss_avg = RunningMean()\n",
    "\n",
    "    # --------- ENTRENAMIENTO ---------\n",
    "    model.train()\n",
    "    loss_train = train(loader_u, loader_v, loader_p, loader_eqns_ref, loader_eqns)\n",
    "    \n",
    "     # --------- VALIDACIÓN (SIN GRADIENTES) ---------\n",
    "    model.eval()\n",
    "    NS_loss, U_loss, V_loss, P_loss = eval(loader_u, loader_v, loader_p, loader_eqns_ref, loader_eqns)\n",
    "    \n",
    "    # End epoch\n",
    "    train_loss_results.append(loss_train)\n",
    "    NS_loss_results.append(NS_loss)\n",
    "    U_loss_results.append(U_loss)\n",
    "    V_loss_results.append(V_loss)\n",
    "    P_loss_results.append(P_loss)\n",
    "\n",
    "    # Update learning rate\n",
    "    new_lr = adjust_learning_rate(model_optimizer, loss_train)\n",
    "    \n",
    "    \n",
    "    print(f\"Epoch: {epoch:4} | \"\n",
    "          f\"Loss training: {loss_train:10.4e} | \"\n",
    "          f\"NS_Loss: {NS_loss:10.4e} | \"\n",
    "          f\"U_Loss: {U_loss:10.4e} | \"\n",
    "          f\"V_Loss: {V_loss:10.4e} | \"\n",
    "          f\"P_Loss: {P_loss:10.4e} | \"\n",
    "          f\"learning rate: {new_lr:10.4e} | \"\n",
    "          )\n",
    "\n",
    "    print(\"\\n=== GRADIENTS ===\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            print(f\"{name:20s} | grad.mean={param.grad.mean():+.4e} | grad.std={param.grad.std():.4e}\")\n",
    "        else:\n",
    "            print(f\"{name:20s} | grad=None\")\n",
    "\n",
    "    # info_model(model)\n",
    "    # input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b2481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"|{0.0001234:10.4e}|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd04dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(10):\n",
    "#     for (t_u,x_u,y_u,u_u), (t_v,x_v,y_v,v_v) in zip(loader_u, loader_v):\n",
    "#         # print(f\"{x_u.shape=}\")\n",
    "#         # print(f\"{y_u=}\")\n",
    "#         # print(f\"{t_u=}\")\n",
    "#         # print(f\"{u_u=}\")\n",
    "#         # print(torch.concat([t_u,x_u,y_u,u_u], axis=1))\n",
    "#         # print(\"*\"*10)\n",
    "#         # print(f\"{x_v.shape=}\")\n",
    "#         # print(f\"{y_v=}\")\n",
    "#         # print(f\"{t_v=}\")\n",
    "#         # print(f\"{v_v=}\")\n",
    "#         print(torch.concat([t_v,x_v,y_v,v_v], axis=1)[:5,:])\n",
    "\n",
    "\n",
    "#         ans = input(\"stop?\")\n",
    "#         if ans == \"y\":\n",
    "#             break\n",
    "#     if ans == \"y\":\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42525d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss_results = []\n",
    "# NS_loss_results = []\n",
    "# P_loss_results = []\n",
    "# U_loss_results = []\n",
    "# V_loss_results = []\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     epoch_loss_avg = RunningMean()\n",
    "#     epoch_NS_loss_avg = RunningMean()\n",
    "#     epoch_P_loss_avg = RunningMean()\n",
    "#     epoch_U_loss_avg = RunningMean()\n",
    "#     epoch_V_loss_avg = RunningMean()\n",
    "\n",
    "#     # zip se detiene en el DataLoader más corto (equivale a tu min_div)\n",
    "#     for (t_u_b, x_u_b, y_u_b, u_u_b), \\\n",
    "#         (t_v_b, x_v_b, y_v_b, v_v_b), \\\n",
    "#         (t_p_b, x_p_b, y_p_b, p_p_b), \\\n",
    "#         (t_eq_ref_b, x_eq_ref_b, y_eq_ref_b), \\\n",
    "#         (t_eq_b, x_eq_b, y_eq_b) in zip(loader_u, loader_v, loader_p, loader_eqns_ref, loader_eqns):\n",
    "\n",
    "#         # Enviar a device\n",
    "#         t_u_b, x_u_b, y_u_b, u_u_b = to_dev(t_u_b, x_u_b, y_u_b, u_u_b, device=device)\n",
    "#         t_v_b, x_v_b, y_v_b, v_v_b = to_dev(t_v_b, x_v_b, y_v_b, v_v_b, device=device)\n",
    "#         t_p_b, x_p_b, y_p_b, p_p_b = to_dev(t_p_b, x_p_b, y_p_b, p_p_b, device=device)\n",
    "#         t_eq_ref_b, x_eq_ref_b, y_eq_ref_b = to_dev(t_eq_ref_b, x_eq_ref_b, y_eq_ref_b, device=device)\n",
    "#         t_eq_b, x_eq_b, y_eq_b = to_dev(t_eq_b, x_eq_b, y_eq_b, device=device)\n",
    "\n",
    "#         # Paso de entrenamiento\n",
    "#         model_optimizer.zero_grad(set_to_none=True)\n",
    "#         loss_train = loss_total(\n",
    "#             model,\n",
    "#             # u\n",
    "#             t_u_b, x_u_b, y_u_b, u_u_b,\n",
    "#             # v\n",
    "#             t_v_b, x_v_b, y_v_b, v_v_b,\n",
    "#             # p\n",
    "#             t_p_b, x_p_b, y_p_b, p_p_b,\n",
    "#             # eqns ref\n",
    "#             t_eq_ref_b, x_eq_ref_b, y_eq_ref_b,\n",
    "#             # eqns\n",
    "#             t_eq_b, x_eq_b, y_eq_b,\n",
    "#             lamb, training=True\n",
    "#         )\n",
    "#         loss_train.backward()\n",
    "#         model_optimizer.step()\n",
    "\n",
    "#         # Métricas por lote (sin gradiente, modo eval para consistencia)\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             NS_loss = loss_NS_2D(model, t_eq_b.clone(), x_eq_b.clone(), y_eq_b.clone(), training=False)\n",
    "#             P_loss = loss_p(model, t_p_b, x_p_b, y_p_b, p_p_b, training=False)\n",
    "#             U_loss = loss_u(model, t_u_b, x_u_b, y_u_b, u_u_b, training=False)\n",
    "#             V_loss = loss_v(model, t_v_b, x_v_b, y_v_b, v_v_b, training=False)\n",
    "\n",
    "#         # Acumular promedios\n",
    "#         bs = t_u_b.shape[0]  # cualquier batch size como peso\n",
    "#         epoch_loss_avg.update(loss_train, n=bs)\n",
    "#         epoch_NS_loss_avg.update(NS_loss, n=bs)\n",
    "#         epoch_P_loss_avg.update(P_loss, n=bs)\n",
    "#         epoch_U_loss_avg.update(U_loss, n=bs)\n",
    "#         epoch_V_loss_avg.update(V_loss, n=bs)\n",
    "\n",
    "#         model.train()  # volver a train para el siguiente batch\n",
    "\n",
    "#     # --- fin de la época: guardar resultados de métricas ---\n",
    "#     train_loss_results.append(epoch_loss_avg.result)\n",
    "#     NS_loss_results.append(epoch_NS_loss_avg.result)\n",
    "#     P_loss_results.append(epoch_P_loss_avg.result)\n",
    "#     U_loss_results.append(epoch_U_loss_avg.result)\n",
    "#     V_loss_results.append(epoch_V_loss_avg.result)\n",
    "\n",
    "#     # --- actualizar LR como en tu scheduler por umbrales ---\n",
    "#     avg = epoch_loss_avg.result\n",
    "#     if avg > 1e-1:\n",
    "#         new_lr = 1e-3\n",
    "#     elif avg > 3e-2:\n",
    "#         new_lr = 1e-4\n",
    "#     elif avg > 3e-3:\n",
    "#         new_lr = 1e-5\n",
    "#     else:\n",
    "#         new_lr = 1e-6\n",
    "#     for g in model_optimizer.param_groups:\n",
    "#         g['lr'] = new_lr\n",
    "\n",
    "#     print(f\"Epoch: {epoch:4d} \"\n",
    "#           f\"Loss_training: {epoch_loss_avg.result:.3e} \"\n",
    "#           f\"NS_loss: {epoch_NS_loss_avg.result:.3e} \"\n",
    "#           f\"P_loss: {epoch_P_loss_avg.result:.3e} \"\n",
    "#           f\"U_loss: {epoch_U_loss_avg.result:.3e} \"\n",
    "#           f\"V_loss: {epoch_V_loss_avg.result:.3e}  \"\n",
    "#           f\"(lr={new_lr:.0e})\")\n",
    "\n",
    "#     # ------------------ Guardado de predicciones/modelo ------------------\n",
    "#     if (epoch + 1) % num_epochs == 0:\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             # Salidas de alta resolución (PINN grid)\n",
    "#             U_PINN = np.zeros_like(X_PINN)\n",
    "#             V_PINN = np.zeros_like(X_PINN)\n",
    "#             P_PINN = np.zeros_like(X_PINN)\n",
    "\n",
    "#             for snap in range(0, X_PINN.shape[1]):\n",
    "#                 t_out = torch.as_tensor(T_PINN[:, snap:snap+1], dtype=torch.get_default_dtype(), device=device)\n",
    "#                 x_out = torch.as_tensor(X_PINN[:, snap:snap+1], dtype=torch.get_default_dtype(), device=device)\n",
    "#                 y_out = torch.as_tensor(Y_PINN[:, snap:snap+1], dtype=torch.get_default_dtype(), device=device)\n",
    "#                 X_out = torch.cat([t_out, x_out, y_out], dim=1)  # [N,3]\n",
    "#                 Y_out = model(X_out)                              # [N,3]\n",
    "#                 u_pred, v_pred, p_pred = Y_out[:,0:1], Y_out[:,1:2], Y_out[:,2:3]\n",
    "#                 U_PINN[:, snap:snap+1] = u_pred.cpu().numpy()\n",
    "#                 V_PINN[:, snap:snap+1] = v_pred.cpu().numpy()\n",
    "#                 P_PINN[:, snap:snap+1] = p_pred.cpu().numpy()\n",
    "\n",
    "#             # Predicciones en WS\n",
    "#             U_WS_pred = np.zeros_like(X_WS)\n",
    "#             V_WS_pred = np.zeros_like(X_WS)\n",
    "#             P_WS_pred = np.zeros_like(X_WS)\n",
    "\n",
    "#             for snap in range(0, X_WS.shape[1]):\n",
    "#                 t_out = torch.as_tensor(T_WS[:, snap:snap+1], dtype=torch.get_default_dtype(), device=device)\n",
    "#                 x_out = torch.as_tensor(X_WS[:, snap:snap+1], dtype=torch.get_default_dtype(), device=device)\n",
    "#                 y_out = torch.as_tensor(Y_WS[:, snap:snap+1], dtype=torch.get_default_dtype(), device=device)\n",
    "#                 X_out = torch.cat([t_out, x_out, y_out], dim=1)\n",
    "#                 Y_out = model(X_out)\n",
    "#                 u_pred, v_pred, p_pred = Y_out[:,0:1], Y_out[:,1:2], Y_out[:,2:3]\n",
    "#                 U_WS_pred[:, snap:snap+1] = u_pred.cpu().numpy()\n",
    "#                 V_WS_pred[:, snap:snap+1] = v_pred.cpu().numpy()\n",
    "#                 P_WS_pred[:, snap:snap+1] = p_pred.cpu().numpy()\n",
    "\n",
    "#             # Predicciones en validación\n",
    "#             U_val_pred = np.zeros_like(X_val)\n",
    "#             V_val_pred = np.zeros_like(X_val)\n",
    "#             P_val_pred = np.zeros_like(X_val)\n",
    "\n",
    "#             for snap in range(0, X_val.shape[1]):\n",
    "#                 t_out = torch.as_tensor(T_val[:, snap:snap+1], dtype=torch.get_default_dtype(), device=device)\n",
    "#                 x_out = torch.as_tensor(X_val[:, snap:snap+1], dtype=torch.get_default_dtype(), device=device)\n",
    "#                 y_out = torch.as_tensor(Y_val[:, snap:snap+1], dtype=torch.get_default_dtype(), device=device)\n",
    "#                 X_out = torch.cat([t_out, x_out, y_out], dim=1)\n",
    "#                 Y_out = model(X_out)\n",
    "#                 u_pred, v_pred, p_pred = Y_out[:,0:1], Y_out[:,1:2], Y_out[:,2:3]\n",
    "#                 U_val_pred[:, snap:snap+1] = u_pred.cpu().numpy()\n",
    "#                 V_val_pred[:, snap:snap+1] = v_pred.cpu().numpy()\n",
    "#                 P_val_pred[:, snap:snap+1] = p_pred.cpu().numpy()\n",
    "\n",
    "#         # Guardar .mat\n",
    "#         scipy.io.savemat(\n",
    "#             f'Brussels_{epoch+1}_lambda_{lamb}_R_{R}_envelope.mat',\n",
    "#             {\n",
    "#                 'T_PINN': T_PINN, 'X_PINN': X_PINN, 'Y_PINN': Y_PINN,\n",
    "#                 'U_PINN': U_PINN, 'V_PINN': V_PINN, 'P_PINN': P_PINN,\n",
    "#                 'T_WS': T_WS, 'X_WS': X_WS, 'Y_WS': Y_WS,\n",
    "#                 'U_WS': U_WS, 'V_WS': V_WS, 'P_WS': P_WS,\n",
    "#                 'U_WS_pred': U_WS_pred, 'V_WS_pred': V_WS_pred, 'P_WS_pred': P_WS_pred,\n",
    "#                 'T_val': T_val, 'X_val': X_val, 'Y_val': Y_val,\n",
    "#                 'U_val': U_val, 'V_val': V_val, 'P_val': P_val,\n",
    "#                 'U_val_pred': U_val_pred, 'V_val_pred': V_val_pred, 'P_val_pred': P_val_pred,\n",
    "#                 'Train_loss': np.array(train_loss_results, dtype=float),\n",
    "#                 'NS_loss': np.array(NS_loss_results, dtype=float),\n",
    "#                 'P_loss': np.array(P_loss_results, dtype=float),\n",
    "#                 'U_loss': np.array(U_loss_results, dtype=float),\n",
    "#                 'V_loss': np.array(V_loss_results, dtype=float),\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "#         # Guardar el modelo (state dict + versión trazada opcional)\n",
    "#         model_filename = f'PINN_model_epoch_{epoch+1}_lambda_{lamb}_R_{R}'\n",
    "#         torch.save(model.state_dict(), model_filename + \".pth\")\n",
    "#         try:\n",
    "#             # Trazeo con un input dummy (ajusta el tamaño según tu caso)\n",
    "#             dummy = torch.zeros(1, 3, device=device, dtype=torch.get_default_dtype())\n",
    "#             traced = torch.jit.trace(model, dummy)\n",
    "#             traced.save(model_filename + \"_traced.pt\")\n",
    "#         except Exception as e:\n",
    "#             print(\"Aviso: no se pudo trazar el modelo (torch.jit.trace).\", e)\n",
    "#         print(f\"Modelo guardado en: {model_filename}.pth (y traced si fue posible)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f353e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264cbe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bd23b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959fde47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d7b2e2",
   "metadata": {},
   "source": [
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b569683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "\n",
    "# # 1. Define a custom Dataset\n",
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, data):\n",
    "#         self.data = data\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.data[idx]\n",
    "\n",
    "# # Create some dummy data\n",
    "# dummy_data = [f\"sample_{i}\" for i in range(10)]\n",
    "# dataset = CustomDataset(dummy_data)\n",
    "\n",
    "# # 2. Create a RandomSampler\n",
    "# # By default, replacement is False (sampling without replacement)\n",
    "# # You can also specify num_samples if you want to sample a subset\n",
    "# sampler = RandomSampler(dataset, replacement=False) \n",
    "\n",
    "# # If you want to sample with replacement and specify a number of samples:\n",
    "# # sampler_with_replacement = RandomSampler(dataset, replacement=True, num_samples=20) \n",
    "\n",
    "# # 3. Create a DataLoader using the sampler\n",
    "# # When a sampler is provided, the 'shuffle' argument in DataLoader should be False\n",
    "# dataloader = DataLoader(dataset, batch_size=5, sampler=sampler)\n",
    "\n",
    "# # 4. Iterate through the DataLoader to get random batches\n",
    "# print(\"Randomly sampled batches:\")\n",
    "# for batch in dataloader:\n",
    "#     print(batch)\n",
    "\n",
    "# # Example with a fixed random seed for reproducibility\n",
    "# print(\"\\nRandomly sampled batches with fixed seed:\")\n",
    "# generator = torch.Generator()\n",
    "# generator.manual_seed(42) # Set a seed for reproducibility\n",
    "# sampler_seeded = RandomSampler(dataset, generator=generator)\n",
    "# dataloader_seeded = DataLoader(dataset, batch_size=2, sampler=sampler_seeded)\n",
    "\n",
    "# for batch in dataloader_seeded:\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb63e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.choice(10, 8, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c5e6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
