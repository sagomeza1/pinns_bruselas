{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90c993fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "import scipy.io as sio\n",
    "from pathlib import Path\n",
    "# from torch.nn.utils import weight_norm\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd2b68ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.3\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ef6bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando PyTorch 2.9.0+cu128 | GPU disponible: True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando PyTorch {torch.__version__} | GPU disponible: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21d922cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Opcional) ligeras optimizaciones en GPU\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b1ce95",
   "metadata": {},
   "source": [
    "# Rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa38cd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path().resolve().parents[0]\n",
    "DATA_PROC_DIR = BASE_DIR / \"data\" / \"processed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4443c9fa",
   "metadata": {},
   "source": [
    "# Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "476e45b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = DATA_PROC_DIR / \"weather_data_processed.mat\"\n",
    "WS_data = sio.loadmat(path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40a84e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Malla PINN\n",
    "X_PINN = WS_data[\"X_PINN\"]\n",
    "Y_PINN = WS_data[\"Y_PINN\"]\n",
    "T_PINN = WS_data[\"T_PINN\"]\n",
    "# Data WS\n",
    "T_WS = WS_data[\"T_WS\"]\n",
    "X_WS = WS_data[\"X_WS\"]\n",
    "Y_WS = WS_data[\"Y_WS\"]\n",
    "U_WS = WS_data[\"U_WS\"]\n",
    "V_WS = WS_data[\"V_WS\"]\n",
    "P_WS = WS_data[\"P_WS\"]\n",
    "# Data val\n",
    "WS_val = WS_data[\"WS_val\"]\n",
    "T_val = WS_data[\"T_val\"]\n",
    "P_val = WS_data[\"P_val\"]\n",
    "U_val = WS_data[\"U_val\"]\n",
    "V_val = WS_data[\"V_val\"]\n",
    "X_val = WS_data[\"X_val\"]\n",
    "Y_val = WS_data[\"Y_val\"]\n",
    "Z_val = WS_data[\"Z_val\"]\n",
    "\n",
    "L = WS_data[\"L\"]\n",
    "W = WS_data[\"W\"]\n",
    "P0 = WS_data[\"P0\"]\n",
    "Re = WS_data[\"Re\"]\n",
    "\n",
    "batch_PINN = WS_data[\"batch_PINN\"][0][0]\n",
    "batch_WS = WS_data[\"batch_WS\"][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77d3bb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(4608)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_PINN#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbc5a928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.nanmax(T_WS)=np.float64(49.32331232012715) , np.nanmin(T_WS)=np.float64(0.0)\n",
      "np.nanmax(P_WS)=np.float64(7.912132117520519) , np.nanmin(P_WS)=np.float64(-4.412815666525302)\n",
      "np.nanmax(U_WS)=np.float64(0.7578370168417706) , np.nanmin(U_WS)=np.float64(-0.7057915726090886)\n",
      "np.nanmax(T_WS)=np.float64(49.32331232012715) , np.nanmin(T_WS)=np.float64(0.0)\n",
      "np.nanmax(X_WS)=np.float64(0.4778002700915572) , np.nanmin(X_WS)=np.float64(-0.4778002700915572)\n",
      "np.nanmax(Y_WS)=np.float64(0.14733262334064023) , np.nanmin(Y_WS)=np.float64(-0.14733262334064023)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{np.nanmax(T_WS)=} , {np.nanmin(T_WS)=}\")\n",
    "print(f\"{np.nanmax(P_WS)=} , {np.nanmin(P_WS)=}\")\n",
    "print(f\"{np.nanmax(U_WS)=} , {np.nanmin(U_WS)=}\")\n",
    "print(f\"{np.nanmax(T_WS)=} , {np.nanmin(T_WS)=}\")\n",
    "print(f\"{np.nanmax(X_WS)=} , {np.nanmin(X_WS)=}\")\n",
    "print(f\"{np.nanmax(Y_WS)=} , {np.nanmin(Y_WS)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec9696b",
   "metadata": {},
   "source": [
    "# Utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d427e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda')\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"{device=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abbe29f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "num_epochs = 1000 # number of epochs\n",
    "lamb = 2 # Tuning of physics constraints\n",
    "# dtype = np.float64\n",
    "dtype = np.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8643311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d906c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _flatten_grid(T, X, Y, target=None):\n",
    "    \"\"\"\n",
    "    Recibe matrices 2D (N_x by N_t) y devuelve vectores columna aplanados (N_x*N_t, 1).\n",
    "    Aplica máscara de NaN con base en `target` si se provee.\n",
    "    \"\"\"\n",
    "    # Aplanar\n",
    "    t = T.reshape(-1, 1)\n",
    "    x = X.reshape(-1, 1)\n",
    "    y = Y.reshape(-1, 1)\n",
    "    tgt = None if target is None else target.reshape(-1, 1)\n",
    "\n",
    "    # Máscara de NaN según target si existe\n",
    "    if tgt is not None:\n",
    "        mask = ~np.isnan(tgt[:, 0])\n",
    "        t, x, y = t[mask], x[mask], y[mask]\n",
    "        tgt = tgt[mask]\n",
    "    return t, x, y, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91a74397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_device(batch, device):\n",
    "    if isinstance(batch, (list, tuple)):\n",
    "        return [item.to(device, non_blocking=True) for item in batch]\n",
    "    elif isinstance(batch, dict):\n",
    "        return {key: val.to(device, non_blocking=True) for key, val in batch.items()}\n",
    "    else:\n",
    "        return batch.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2be6054",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72b854ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset para un campo escalar (u, v o p) en las ubicaciones WS (t, x, y [,z]).\n",
    "    Permite barajar con DataLoader + RandomSampler.\n",
    "    \"\"\"\n",
    "    def __init__(self, T_WS, X_WS, Y_WS, target_WS):\n",
    "        t, x, y, tgt = _flatten_grid(T_WS, X_WS, Y_WS, target_WS)\n",
    "        self.t = torch.from_numpy(t.astype(dtype))\n",
    "        self.x = torch.from_numpy(x.astype(dtype))\n",
    "        self.y = torch.from_numpy(y.astype(dtype))\n",
    "        self.target = torch.from_numpy(tgt.astype(dtype))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.t.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return torch.concat([self.t[idx], self.x[idx], self.y[idx], self.target[idx]], axis=1)\n",
    "        return self.t[idx], self.x[idx], self.y[idx], self.target[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20bb1796",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSEqnRefDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset para (t, x, y) de referencia desde WS (para términos de ecuaciones que usan ref).\n",
    "    \"\"\"\n",
    "    def __init__(self, T_WS, X_WS, Y_WS):\n",
    "        t, x, y, _ = _flatten_grid(T_WS, X_WS, Y_WS)\n",
    "        self.t = torch.from_numpy(t.astype(dtype))\n",
    "        self.x = torch.from_numpy(x.astype(dtype))\n",
    "        self.y = torch.from_numpy(y.astype(dtype))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.t.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return torch.concat([self.t[idx], self.x[idx], self.y[idx]], axis=1)\n",
    "        return self.t[idx], self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd0a7e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINNEqnDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset de puntos de colación (PINN) provenientes de malla PINN (sin targets).\n",
    "    \"\"\"\n",
    "    def __init__(self, T_PINN, X_PINN, Y_PINN):\n",
    "        t, x, y, _ = _flatten_grid(T_PINN, X_PINN, Y_PINN)\n",
    "        self.t = torch.from_numpy(t.astype(dtype))\n",
    "        self.x = torch.from_numpy(x.astype(dtype))\n",
    "        self.y = torch.from_numpy(y.astype(dtype))\n",
    "        # print(self.t.shape)\n",
    "        # print(self.x.shape)\n",
    "        # print(self.y.shape)\n",
    "        # print(self.t.shape[0])\n",
    "        # print(torch.concat([self.t, self.x, self.y], axis=1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.t.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return torch.concat([self.t[idx,:], self.x[idx,:], self.y[idx,:]], axis=1)\n",
    "        return self.t[idx], self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b30029",
   "metadata": {},
   "source": [
    "# Preparación de datasets y dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cc705c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "dim_N_WS = X_WS.shape[0]\n",
    "dim_T_WS = X_WS.shape[1]\n",
    "dim_N_PINN = X_PINN.shape[0]\n",
    "dim_T_PINN = T_PINN.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3623e854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_PINN=4608, batch_WS=231\n"
     ]
    }
   ],
   "source": [
    "# Dimensions\n",
    "dim_N_data = dim_N_WS\n",
    "dim_T_data = dim_T_WS\n",
    "dim_T_eqns = dim_T_PINN\n",
    "dim_N_eqns = dim_N_PINN\n",
    "\n",
    "num_samples_WS = int(dim_N_data * dim_T_data)\n",
    "num_samples_PINN = int(dim_N_eqns * dim_T_eqns)\n",
    "\n",
    "batch_PINN = int(batch_PINN)\n",
    "batch_WS = int(batch_WS)\n",
    "\n",
    "print(f\"{batch_PINN=}, {batch_WS=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d05228cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjuntos\n",
    "ds_u = WSDataset(T_WS, X_WS, Y_WS, U_WS)\n",
    "ds_v = WSDataset(T_WS, X_WS, Y_WS, V_WS)\n",
    "ds_p = WSDataset(T_WS, X_WS, Y_WS, P_WS)\n",
    "\n",
    "ds_eqns_ref = WSEqnRefDataset(T_WS, X_WS, Y_WS)\n",
    "ds_eqns = PINNEqnDataset(T_PINN, X_PINN, Y_PINN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "975313e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestreo aleatorio por época\n",
    "sampler_u = RandomSampler(ds_u, replacement=False, num_samples=num_samples_WS)\n",
    "sampler_v = RandomSampler(ds_v, replacement=False, num_samples=num_samples_WS)\n",
    "sampler_p = RandomSampler(ds_p, replacement=False, num_samples=num_samples_WS)\n",
    "sampler_eqns_ref = RandomSampler(ds_eqns_ref, replacement=False, num_samples=num_samples_WS)\n",
    "sampler_eqns = RandomSampler(ds_eqns, replacement=False, num_samples=num_samples_PINN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ad80d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de los dataloaders con el muestreo simple\n",
    "loader_u = DataLoader(ds_u, batch_size=batch_WS, sampler=sampler_u, pin_memory=True)\n",
    "loader_v = DataLoader(ds_v, batch_size=batch_WS, sampler=sampler_v, pin_memory=True)\n",
    "loader_p = DataLoader(ds_p, batch_size=batch_WS, sampler=sampler_p, pin_memory=True)\n",
    "loader_eqns_ref = DataLoader(ds_eqns_ref, batch_size=batch_WS, sampler=sampler_eqns_ref, pin_memory=True)\n",
    "loader_eqns = DataLoader(ds_eqns, batch_size=batch_PINN, sampler=sampler_eqns, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bf2b65",
   "metadata": {},
   "source": [
    "# Capa personalizada: GammaBiasLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59dd04c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GammaBiasLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Capa densa personalizada:\n",
    "      y = gamma ⊙ (W_norm x) + bias\n",
    "    - W_norm: Linear (sin bias) con Weight Normalization\n",
    "    - gamma: parámetro de escala por-neurona\n",
    "    - bias: sesgo por-neurona\n",
    "\n",
    "    Args:\n",
    "        in_features  (int): tamaño de entrada\n",
    "        out_features (int): número de unidades (neuronas)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        # Linear sin bias para emular Dense(use_bias=False)\n",
    "        linear = nn.Linear(in_features, out_features, bias=False)\n",
    "        # Inicialización uniforme [-1, 1], como en tu RandomUniform\n",
    "        nn.init.uniform_(linear.weight, a=-1.0, b=1.0)\n",
    "        \n",
    "        # Weight Normalization (equivalente a tfa.layers.WeightNormalization)\n",
    "        self.w = weight_norm(linear)  # añade weight_g y weight_v internamente\n",
    "\n",
    "        # Parámetros gamma y bias (forma [out_features])\n",
    "        self.gamma = nn.Parameter(torch.ones(out_features))\n",
    "        self.bias  = nn.Parameter(torch.zeros(out_features))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (batch, in_features) -> out: (batch, out_features)\n",
    "        y = self.w(x)\n",
    "        # Broadcasting de gamma y bias sobre la dimensión batch\n",
    "        return y * self.gamma + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7455f9c",
   "metadata": {},
   "source": [
    "# PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7cf944",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class PINNNet(nn.Module):\n",
    "#     def __init__(self, num_input_variables=3, num_output_variables=3):\n",
    "#         super().__init__()\n",
    "#         neurons = 200 * num_output_variables\n",
    "#         hidden_sizes = (2 * (num_input_variables + num_output_variables))*[neurons]\n",
    "#         layers = []\n",
    "#         last = num_input_variables\n",
    "#         for h in hidden_sizes + [num_output_variables]:\n",
    "#                      layers.append(nn.Linear(last, h, bias=True))\n",
    "#                      last = h\n",
    "#         self.layers = nn.ModuleList(layers)\n",
    "#         self.activation = nn.Tanh()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Aplicar activación solo en las primeras 5 capas ocultas\n",
    "#         for i, layer in enumerate(self.layers):\n",
    "#             x = layer(x)\n",
    "#             if i < 8:  # activación en capas 0 a 4\n",
    "#                          x = self.activation(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc4368",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PINNNet(nn.Module):\n",
    "    def __init__(self, num_input_variables=3, num_output_variables=3):\n",
    "        super().__init__()\n",
    "        neurons = 200 * num_output_variables\n",
    "        hidden_sizes = (2 * (num_input_variables + num_output_variables))*[neurons]\n",
    "        layers = []\n",
    "        last = num_input_variables\n",
    "        for h in hidden_sizes + [num_output_variables]:\n",
    "                     layers.append(GammaBiasLayer(last, h, bias=True))\n",
    "                     last = h\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Aplicar activación solo en las primeras 5 capas ocultas\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if i < 8:  # activación en capas 0 a 4\n",
    "                         x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "effcfe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class PINNNet(nn.Module):\n",
    "#     def __init__(self, num_input_variables=3, num_output_variables=3):\n",
    "#         super().__init__()\n",
    "#         self.activation = nn.Tanh()\n",
    "#         self.l01 = nn.Linear(3, 600, bias=True)\n",
    "#         self.l02 = nn.Linear(600, 600, bias=True)\n",
    "#         self.l03 = nn.Linear(600, 600, bias=True)\n",
    "#         self.l04 = nn.Linear(600, 600, bias=True)\n",
    "#         self.l05 = nn.Linear(600, 600, bias=True)\n",
    "#         self.l06 = nn.Linear(600, 600, bias=True)\n",
    "#         self.l07 = nn.Linear(600, 600, bias=True)\n",
    "#         self.l08 = nn.Linear(600, 600, bias=True)\n",
    "#         self.l09 = nn.Linear(600, 600, bias=True)\n",
    "#         self.l10 = nn.Linear(600, 600, bias=True)\n",
    "#         self.l11 = nn.Linear(600, 600, bias=True)\n",
    "#         self.l12 = nn.Linear(600, 600, bias=True)\n",
    "#         self.lfi = nn.Linear(600, 3, bias=True)\n",
    "\n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         a01 = self.activation(self.l01(x))\n",
    "#         a02 = self.activation(self.l02(a01))\n",
    "#         a03 = self.activation(self.l03(a02))\n",
    "#         a04 = self.activation(self.l04(a03))\n",
    "#         a05 = self.activation(self.l05(a04))\n",
    "#         a06 = self.activation(self.l06(a05))\n",
    "#         a07 = self.activation(self.l07(a06))\n",
    "#         a08 = self.activation(self.l08(a07))\n",
    "#         a09 = self.l09(a08)\n",
    "#         a10 = self.l10(a09)\n",
    "#         a11 = self.l11(a10)\n",
    "#         a12 = self.l12(a11)\n",
    "#         afi = self.lfi(a12)\n",
    "\n",
    "\n",
    "#         # return afi, (a01, a02, a03, a04, a05, a06, a07, a08, a09, a10, a11, a12, afi, )\n",
    "#         return afi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b71d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class PINNNet(nn.Module):\n",
    "#     def __init__(self, num_input_variables=3, num_output_variables=3):\n",
    "#         super().__init__()\n",
    "#         self.activation = nn.Tanh()\n",
    "#         self.l01 = GammaBiasLayer(3, 600)\n",
    "#         self.l02 = GammaBiasLayer(600, 600)\n",
    "#         self.l03 = GammaBiasLayer(600, 600)\n",
    "#         self.l04 = GammaBiasLayer(600, 600)\n",
    "#         self.l05 = GammaBiasLayer(600, 600)\n",
    "#         self.l06 = GammaBiasLayer(600, 600)\n",
    "#         self.l07 = GammaBiasLayer(600, 600)\n",
    "#         self.l08 = GammaBiasLayer(600, 600)\n",
    "#         self.l09 = GammaBiasLayer(600, 600)\n",
    "#         self.l10 = GammaBiasLayer(600, 600)\n",
    "#         self.l11 = GammaBiasLayer(600, 600)\n",
    "#         self.l12 = GammaBiasLayer(600, 600)\n",
    "#         self.lfi = GammaBiasLayer(600, 3)\n",
    "\n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         a01 = self.activation(self.l01(x))\n",
    "#         a02 = self.activation(self.l02(a01))\n",
    "#         a03 = self.activation(self.l03(a02))\n",
    "#         a04 = self.activation(self.l04(a03))\n",
    "#         a05 = self.activation(self.l05(a04))\n",
    "#         a06 = self.activation(self.l06(a05))\n",
    "#         a07 = self.activation(self.l07(a06))\n",
    "#         a08 = self.activation(self.l08(a07))\n",
    "#         a09 = self.l09(a08)\n",
    "#         a10 = self.l10(a09)\n",
    "#         a11 = self.l11(a10)\n",
    "#         a12 = self.l12(a11)\n",
    "#         afi = self.lfi(a12)\n",
    "\n",
    "\n",
    "#         return afi, (a01, a02, a03, a04, a05, a06, a07, a08, a09, a10, a11, a12, afi, )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ce0e027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PINNNet().to(device).double()\n",
    "model = PINNNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a7eb7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  'named_parameters',\n",
    "#  'parameters',\n",
    "def info_model(model):\n",
    "    for i in model.named_parameters(): \n",
    "        print(20*\"#\")\n",
    "        print(f\"Parámetro: {i[0]:10}\")\n",
    "        print(f\"Dim parámetro: {i[1].shape}\")\n",
    "        print(f\"Dim parámetro: {i[1][:5]}\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03aa7805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Parámetro: layers.0.weight\n",
      "Dim parámetro: torch.Size([600, 3])\n",
      "Dim parámetro: tensor([[ 0.1451,  0.2998,  0.1536],\n",
      "        [ 0.0187,  0.0526,  0.4856],\n",
      "        [ 0.2927,  0.4401,  0.4100],\n",
      "        [ 0.3162,  0.5045, -0.2407],\n",
      "        [-0.1035,  0.2253,  0.1435]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: layers.0.bias\n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([ 0.2326, -0.4944, -0.5626, -0.4268, -0.3383], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: layers.1.weight\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[ 0.0171,  0.0146,  0.0276,  ..., -0.0329, -0.0338, -0.0092],\n",
      "        [-0.0305,  0.0108, -0.0365,  ..., -0.0299,  0.0290, -0.0323],\n",
      "        [-0.0185,  0.0372,  0.0127,  ..., -0.0248,  0.0050,  0.0178],\n",
      "        [-0.0151, -0.0064, -0.0029,  ..., -0.0369,  0.0228, -0.0180],\n",
      "        [ 0.0305, -0.0029,  0.0006,  ..., -0.0134, -0.0011, -0.0299]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: layers.1.bias\n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([-0.0001,  0.0202,  0.0106, -0.0094, -0.0043], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: layers.2.weight\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.0146, -0.0034,  0.0261,  ..., -0.0359, -0.0328,  0.0040],\n",
      "        [-0.0222, -0.0090, -0.0348,  ...,  0.0170, -0.0359, -0.0194],\n",
      "        [ 0.0311, -0.0186, -0.0032,  ...,  0.0371, -0.0384,  0.0323],\n",
      "        [ 0.0303,  0.0003,  0.0345,  ..., -0.0316, -0.0163,  0.0056],\n",
      "        [ 0.0031,  0.0336,  0.0172,  ..., -0.0154,  0.0016,  0.0108]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: layers.2.bias\n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([-0.0021,  0.0338, -0.0010,  0.0285,  0.0137], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: layers.3.weight\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[ 0.0241, -0.0074,  0.0049,  ...,  0.0094,  0.0398,  0.0068],\n",
      "        [-0.0275, -0.0242,  0.0082,  ..., -0.0072, -0.0170, -0.0333],\n",
      "        [ 0.0115,  0.0280, -0.0359,  ..., -0.0308, -0.0025, -0.0299],\n",
      "        [-0.0159,  0.0263,  0.0057,  ...,  0.0094,  0.0389,  0.0244],\n",
      "        [-0.0244, -0.0059,  0.0312,  ..., -0.0045,  0.0291, -0.0098]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: layers.3.bias\n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([-0.0140, -0.0026, -0.0215, -0.0252,  0.0315], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: layers.4.weight\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.0149,  0.0173, -0.0325,  ...,  0.0110,  0.0326, -0.0180],\n",
      "        [ 0.0296,  0.0348,  0.0064,  ...,  0.0325, -0.0050, -0.0087],\n",
      "        [ 0.0063,  0.0042, -0.0274,  ..., -0.0333,  0.0285,  0.0213],\n",
      "        [ 0.0017, -0.0188, -0.0317,  ..., -0.0162, -0.0228, -0.0266],\n",
      "        [-0.0153,  0.0244, -0.0134,  ...,  0.0253,  0.0243, -0.0312]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: layers.4.bias\n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([ 0.0287,  0.0165,  0.0303,  0.0215, -0.0380], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: layers.5.weight\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[ 0.0099,  0.0293,  0.0244,  ..., -0.0165, -0.0178, -0.0257],\n",
      "        [ 0.0257,  0.0367, -0.0248,  ...,  0.0320, -0.0036,  0.0058],\n",
      "        [-0.0143,  0.0331, -0.0105,  ..., -0.0302,  0.0100,  0.0384],\n",
      "        [-0.0357, -0.0264,  0.0060,  ...,  0.0336, -0.0060,  0.0103],\n",
      "        [ 0.0167, -0.0119,  0.0003,  ..., -0.0260, -0.0185, -0.0200]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: layers.5.bias\n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([ 0.0351,  0.0140, -0.0176, -0.0333,  0.0123], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: layers.6.weight\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.0078, -0.0364, -0.0398,  ...,  0.0148,  0.0368,  0.0025],\n",
      "        [-0.0346,  0.0027,  0.0159,  ...,  0.0123, -0.0329, -0.0258],\n",
      "        [-0.0144, -0.0014, -0.0162,  ...,  0.0143,  0.0029,  0.0158],\n",
      "        [-0.0090, -0.0315,  0.0213,  ...,  0.0204, -0.0142,  0.0216],\n",
      "        [-0.0363, -0.0177,  0.0318,  ..., -0.0272, -0.0211, -0.0193]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: layers.6.bias\n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([-0.0379,  0.0324, -0.0132,  0.0243,  0.0400], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: layers.7.weight\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[ 0.0145, -0.0166,  0.0321,  ...,  0.0106,  0.0330,  0.0408],\n",
      "        [-0.0337,  0.0351,  0.0333,  ..., -0.0127, -0.0144, -0.0195],\n",
      "        [-0.0126,  0.0211,  0.0320,  ..., -0.0027, -0.0250,  0.0242],\n",
      "        [ 0.0230,  0.0318, -0.0389,  ..., -0.0355,  0.0291,  0.0231],\n",
      "        [-0.0195,  0.0142, -0.0041,  ...,  0.0133,  0.0278,  0.0303]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: layers.7.bias\n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([ 0.0092,  0.0255, -0.0008,  0.0016,  0.0186], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: layers.8.weight\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.0374,  0.0100, -0.0169,  ..., -0.0147,  0.0285, -0.0005],\n",
      "        [-0.0061,  0.0006, -0.0407,  ..., -0.0330,  0.0125,  0.0019],\n",
      "        [-0.0015, -0.0227, -0.0307,  ..., -0.0111, -0.0127, -0.0085],\n",
      "        [-0.0097,  0.0058,  0.0108,  ...,  0.0273, -0.0169, -0.0225],\n",
      "        [-0.0020,  0.0021,  0.0310,  ..., -0.0371, -0.0225, -0.0109]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: layers.8.bias\n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([ 0.0336, -0.0179, -0.0027,  0.0338,  0.0381], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: layers.9.weight\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[-0.0087, -0.0050, -0.0304,  ...,  0.0249, -0.0041,  0.0292],\n",
      "        [-0.0084, -0.0176,  0.0053,  ...,  0.0136, -0.0327,  0.0306],\n",
      "        [-0.0214,  0.0158,  0.0398,  ...,  0.0329, -0.0112, -0.0076],\n",
      "        [-0.0323,  0.0239,  0.0363,  ...,  0.0370, -0.0299,  0.0099],\n",
      "        [ 0.0193,  0.0072, -0.0223,  ..., -0.0288, -0.0089,  0.0259]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: layers.9.bias\n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([ 0.0376,  0.0269,  0.0163, -0.0292, -0.0030], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: layers.10.weight\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[ 0.0324,  0.0341, -0.0234,  ...,  0.0349, -0.0161,  0.0277],\n",
      "        [-0.0023, -0.0140,  0.0099,  ...,  0.0264,  0.0406,  0.0402],\n",
      "        [-0.0394,  0.0394, -0.0032,  ...,  0.0030,  0.0402, -0.0355],\n",
      "        [-0.0258,  0.0246, -0.0004,  ...,  0.0334,  0.0132, -0.0232],\n",
      "        [-0.0304,  0.0056, -0.0095,  ..., -0.0399,  0.0039, -0.0288]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: layers.10.bias\n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([ 0.0344, -0.0076,  0.0357,  0.0085,  0.0121], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: layers.11.weight\n",
      "Dim parámetro: torch.Size([600, 600])\n",
      "Dim parámetro: tensor([[ 0.0242,  0.0380,  0.0299,  ...,  0.0178, -0.0161,  0.0081],\n",
      "        [ 0.0123,  0.0280,  0.0393,  ..., -0.0395, -0.0320, -0.0316],\n",
      "        [-0.0367,  0.0280,  0.0248,  ..., -0.0066,  0.0381, -0.0010],\n",
      "        [-0.0004, -0.0068,  0.0325,  ...,  0.0160, -0.0121,  0.0316],\n",
      "        [ 0.0156, -0.0386,  0.0060,  ...,  0.0234, -0.0326,  0.0175]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: layers.11.bias\n",
      "Dim parámetro: torch.Size([600])\n",
      "Dim parámetro: tensor([ 0.0144,  0.0241,  0.0191, -0.0199,  0.0092], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: layers.12.weight\n",
      "Dim parámetro: torch.Size([3, 600])\n",
      "Dim parámetro: tensor([[ 0.0319,  0.0169,  0.0040,  ..., -0.0303,  0.0047,  0.0316],\n",
      "        [-0.0083, -0.0021, -0.0170,  ...,  0.0198,  0.0388,  0.0175],\n",
      "        [ 0.0123,  0.0141,  0.0163,  ..., -0.0406,  0.0210, -0.0031]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "####################\n",
      "Parámetro: layers.12.bias\n",
      "Dim parámetro: torch.Size([3])\n",
      "Dim parámetro: tensor([-0.0095, -0.0293,  0.0088], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751a631d",
   "metadata": {},
   "source": [
    "# Funciones de perdidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7686f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = nn.MSELoss()\n",
    "# mse_loss = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "# @torch.enable_grad()\n",
    "# def loss_NS_2D(model, t_eqns, x_eqns, y_eqns):\n",
    "def loss_NS_2D(model, t_eqns, x_eqns, y_eqns):\n",
    "    \"\"\"\n",
    "    Calcula los residuales 2D (incompresible) aproximados:\n",
    "      e1 = u_x + v_y\n",
    "      e2 = u_t + (u u_x + v u_y) + p_x\n",
    "      e3 = v_t + (u v_x + v v_y) + p_y\n",
    "    Devuelve MSE(0, e1) + MSE(0, e2) + MSE(0, e3)\n",
    "    \"\"\"\n",
    "    # Asegurar gradientes con respecto a entradas\n",
    "    for ten in (t_eqns, x_eqns, y_eqns):\n",
    "        ten.requires_grad_(True)\n",
    "\n",
    "    X = torch.cat([t_eqns, x_eqns, y_eqns], dim=1)  # [N, 3]\n",
    "    Y = model(X)                                     # [N, 3]\n",
    "    u, v, p = Y[:, 0:1], Y[:, 1:2], Y[:, 2:3]\n",
    "\n",
    "    ones_u = torch.ones_like(u)\n",
    "    ones_v = torch.ones_like(v)\n",
    "    ones_p = torch.ones_like(p)\n",
    "\n",
    "    # Derivadas primeras\n",
    "    u_t = torch.autograd.grad(u, t_eqns, grad_outputs=ones_u, create_graph=False, retain_graph=True)[0]\n",
    "    v_t = torch.autograd.grad(v, t_eqns, grad_outputs=ones_v, create_graph=False, retain_graph=True)[0]\n",
    "\n",
    "    u_x = torch.autograd.grad(u, x_eqns, grad_outputs=ones_u, create_graph=False, retain_graph=True)[0]\n",
    "    v_x = torch.autograd.grad(v, x_eqns, grad_outputs=ones_v, create_graph=False, retain_graph=True)[0]\n",
    "    p_x = torch.autograd.grad(p, x_eqns, grad_outputs=ones_p, create_graph=False, retain_graph=True)[0]\n",
    "\n",
    "    u_y = torch.autograd.grad(u, y_eqns, grad_outputs=ones_u, create_graph=False, retain_graph=True)[0]\n",
    "    v_y = torch.autograd.grad(v, y_eqns, grad_outputs=ones_v, create_graph=False, retain_graph=True)[0]\n",
    "    p_y = torch.autograd.grad(p, y_eqns, grad_outputs=ones_p, create_graph=False, retain_graph=True)[0]\n",
    "\n",
    "    # Residuales Navier–Stokes (sin términos viscosos, conforme a tu snippet)\n",
    "    e1 = (u_x + v_y)\n",
    "    e2 = (u_t + (u * u_x + v * u_y) + p_x)\n",
    "    e3 = (v_t + (u * v_x + v * v_y) + p_y)\n",
    "\n",
    "    zero = torch.zeros_like(e1)\n",
    "    return (\n",
    "        mse_loss(e1, zero) +\n",
    "        mse_loss(e2, zero) +\n",
    "        mse_loss(e3, zero)\n",
    "    )\n",
    "\n",
    "# @torch.no_grad()\n",
    "def _safe_std(x: torch.Tensor, eps: float = 1e-8):\n",
    "    # std con protección para divisiones por cero\n",
    "    s = torch.std(x)\n",
    "    return s.clamp_min(eps)\n",
    "\n",
    "# def loss_u(t_b, x_b, y_b, u_b):\n",
    "def loss_u(model, t_b, x_b, y_b, u_b):\n",
    "    X = torch.cat([t_b, x_b, y_b], dim=1)\n",
    "    Y = model(X)\n",
    "    u_pred = Y[:, 0:1]\n",
    "    # for i, a in enumerate(actv):\n",
    "    #     print(f\"Capa {i:02d}: shape={tuple(a.shape)}, mean={a.mean():+.4f}, std={a.std():.4f}\")\n",
    "    # print(f\"{X[:5,:]=}\")\n",
    "    # print(f\"{Y[:5,:]=}\")\n",
    "    # print(f\"{_safe_std(u_b) ** 2:=.2f}\")\n",
    "    return mse_loss(u_pred, u_b) / (_safe_std(u_b) ** 2)\n",
    "\n",
    "# def loss_v(t_b, x_b, y_b, v_b):\n",
    "def loss_v(model, t_b, x_b, y_b, v_b):\n",
    "    X = torch.cat([t_b, x_b, y_b], dim=1)\n",
    "    Y = model(X)\n",
    "    v_pred = Y[:, 1:2]\n",
    "    return mse_loss(v_pred, v_b) / (_safe_std(v_b) ** 2)\n",
    "\n",
    "# def loss_p(t_b, x_b, y_b, p_b):\n",
    "def loss_p(model, t_b, x_b, y_b, p_b):\n",
    "    X = torch.cat([t_b, x_b, y_b], dim=1)\n",
    "    Y = model(X)\n",
    "    p_pred = Y[:, 2:3]\n",
    "    return mse_loss(p_pred, p_b) / (_safe_std(p_b) ** 2)\n",
    "\n",
    "def loss_total(\n",
    "    model,\n",
    "    # datos u\n",
    "    t_u_b, x_u_b, y_u_b, u_u_b,\n",
    "    # datos v\n",
    "    t_v_b, x_v_b, y_v_b, v_v_b,\n",
    "    # datos p\n",
    "    t_p_b, x_p_b, y_p_b, p_p_b,\n",
    "    # ecuaciones (referencia + ecuaciones)\n",
    "    t_eqns_ref_b, x_eqns_ref_b, y_eqns_ref_b,\n",
    "    t_eqns_b, x_eqns_b, y_eqns_b,\n",
    "    lamb: float\n",
    "):\n",
    "    NS_eqns = lamb * loss_NS_2D(model, t_eqns_b,     x_eqns_b,     y_eqns_b,   )\n",
    "    NS_data = lamb * loss_NS_2D(model, t_eqns_ref_b, x_eqns_ref_b, y_eqns_ref_b)\n",
    "    P_e = loss_p(model, t_p_b, x_p_b, y_p_b, p_p_b)\n",
    "    U_e = loss_u(model, t_u_b, x_u_b, y_u_b, u_u_b)\n",
    "    V_e = loss_v(model, t_v_b, x_v_b, y_v_b, v_v_b)\n",
    "\n",
    "    total_e = NS_eqns + NS_data + U_e + V_e + P_e\n",
    "\n",
    "    return (NS_eqns ** 2 + NS_data ** 2 + U_e ** 2 + V_e ** 2 + P_e ** 2) / total_e\n",
    "    # return total_e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a3f9a5",
   "metadata": {},
   "source": [
    "# Gradiente y optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "594f5251",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d925d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "def grad_amp(model, model_optimizer, scaler,\n",
    "             t_u_batch, x_u_batch, y_u_batch, u_u_batch,\n",
    "             t_v_batch, x_v_batch, y_v_batch, v_v_batch,\n",
    "             t_p_batch, x_p_batch, y_p_batch, p_p_batch,\n",
    "             t_eqns_ref_batch, x_eqns_ref_batch, y_eqns_ref_batch,\n",
    "             t_eqns_batch, x_eqns_batch, y_eqns_batch,\n",
    "             lamb):\n",
    "    \n",
    "    use_amp = False\n",
    "    with torch.amp.autocast(device_type=device.type, enabled=use_amp):\n",
    "        loss_value = loss_total(model,\n",
    "                                t_u_batch, x_u_batch, y_u_batch, u_u_batch,\n",
    "                                t_v_batch, x_v_batch, y_v_batch, v_v_batch,\n",
    "                                t_p_batch, x_p_batch, y_p_batch, p_p_batch,\n",
    "                                t_eqns_ref_batch, x_eqns_ref_batch, y_eqns_ref_batch,\n",
    "                                t_eqns_batch, x_eqns_batch, y_eqns_batch,\n",
    "                                lamb)\n",
    "\n",
    "    # Escalar la pérdida antes del backward\n",
    "    scaler.scale(loss_value).backward()\n",
    "\n",
    "    # Revisa gradientes ANTES del step\n",
    "    all_finite = True\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.grad is None: \n",
    "            continue\n",
    "        if not torch.isfinite(p.grad).all():\n",
    "            print(f\"[WARN] grad no finito en {n}\")\n",
    "            all_finite = False\n",
    "            break\n",
    "\n",
    "    # (Opcional) Clipping y chequeo de gradientes\n",
    "    scaler.unscale_(model_optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # Paso del optimizador de forma segura\n",
    "    scaler.step(model_optimizer)\n",
    "\n",
    "    # Actualiza el factor de escala automáticamente\n",
    "    scaler.update()\n",
    "\n",
    "    # # Si quieres inspeccionar gradientes:\n",
    "    # grads = [p.grad.detach().clone() if p.grad is not None else None\n",
    "    #          for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "    return loss_value.detach()#, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa5841",
   "metadata": {},
   "source": [
    "# Helpers de métricas simples por época (media acumulada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e114d34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningMean:\n",
    "    def __init__(self):\n",
    "        self.sum = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        # value puede ser tensor o float\n",
    "        v = float(value) if torch.is_tensor(value) else float(value)\n",
    "        self.sum += v * n\n",
    "        self.count += n\n",
    "\n",
    "    @property\n",
    "    def result(self):\n",
    "        return self.sum / max(self.count, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9170e426",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12c6b343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62c3436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch_loss):\n",
    "    if epoch_loss > 1e-1:\n",
    "        new_lr = 1e-3\n",
    "    elif epoch_loss > 3e-2:\n",
    "        new_lr = 1e-4\n",
    "    elif epoch_loss > 3e-3:\n",
    "        new_lr = 1e-5\n",
    "    else:\n",
    "        new_lr = 1e-6\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "\n",
    "    return new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd29282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader_u, loader_v, loader_p, loader_eqns_ref, loader_eqns):\n",
    "\n",
    "    for idx,(\\\n",
    "        (t_u_b, x_u_b, y_u_b, u_u_b), \\\n",
    "        (t_v_b, x_v_b, y_v_b, v_v_b), \\\n",
    "        (t_p_b, x_p_b, y_p_b, p_p_b), \\\n",
    "        (t_eq_ref_b, x_eq_ref_b, y_eq_ref_b), \\\n",
    "        (t_eq_b, x_eq_b, y_eq_b)) in enumerate(zip(loader_u, loader_v, loader_p, loader_eqns_ref, loader_eqns)):\n",
    "        \n",
    "\n",
    "        # Se envian tensores a device\n",
    "        t_u_b = t_u_b.to(device)\n",
    "        x_u_b = x_u_b.to(device)\n",
    "        y_u_b = y_u_b.to(device)\n",
    "        u_u_b = u_u_b.to(device)\n",
    "        t_v_b = t_v_b.to(device)\n",
    "        x_v_b = x_v_b.to(device)\n",
    "        y_v_b = y_v_b.to(device)\n",
    "        v_v_b = v_v_b.to(device)\n",
    "        t_p_b = t_p_b.to(device)\n",
    "        x_p_b = x_p_b.to(device)\n",
    "        y_p_b = y_p_b.to(device)\n",
    "        p_p_b = p_p_b.to(device)\n",
    "        t_eq_ref_b = t_eq_ref_b.to(device)\n",
    "        x_eq_ref_b = x_eq_ref_b.to(device)\n",
    "        y_eq_ref_b = y_eq_ref_b.to(device)\n",
    "        t_eq_b = t_eq_b.to(device)\n",
    "        x_eq_b = x_eq_b.to(device)\n",
    "        y_eq_b = y_eq_b.to(device)\n",
    "\n",
    "\n",
    "        model_optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        use_amp = False\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=use_amp):\n",
    "            loss_value = loss_total(model, t_u_b, x_u_b, y_u_b, u_u_b,\n",
    "                                    t_v_b, x_v_b, y_v_b, v_v_b,\n",
    "                                    t_p_b, x_p_b, y_p_b, p_p_b,\n",
    "                                    t_eq_ref_b, x_eq_ref_b, y_eq_ref_b,\n",
    "                                    t_eq_b, x_eq_b, y_eq_b,\n",
    "                                    lamb)\n",
    "\n",
    "        # Escalar la pérdida antes del backward\n",
    "        scaler.scale(loss_value).backward()\n",
    "\n",
    "        # Revisa gradientes ANTES del step\n",
    "        all_finite = True\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.grad is None: \n",
    "                continue\n",
    "            if not torch.isfinite(p.grad).all():\n",
    "                print(f\"[WARN] grad no finito en {n}\")\n",
    "                all_finite = False\n",
    "                break\n",
    "\n",
    "        # (Opcional) Clipping y chequeo de gradientes\n",
    "        scaler.unscale_(model_optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Paso del optimizador de forma segura\n",
    "        scaler.step(model_optimizer)\n",
    "\n",
    "        # Actualiza el factor de escala automáticamente\n",
    "        scaler.update()\n",
    "        \n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a915ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(loader_u, loader_v, loader_p, loader_eqns_ref, loader_eqns):\n",
    "    for idx,(\\\n",
    "        (t_u_b, x_u_b, y_u_b, u_u_b), \\\n",
    "        (t_v_b, x_v_b, y_v_b, v_v_b), \\\n",
    "        (t_p_b, x_p_b, y_p_b, p_p_b), \\\n",
    "        (t_eq_ref_b, x_eq_ref_b, y_eq_ref_b), \\\n",
    "        (t_eq_b, x_eq_b, y_eq_b)) in enumerate(zip(loader_u, loader_v, loader_p, loader_eqns_ref, loader_eqns)):\n",
    "\n",
    "        # Se envian tensores a device\n",
    "        t_u_b = t_u_b.to(device)\n",
    "        x_u_b = x_u_b.to(device)\n",
    "        y_u_b = y_u_b.to(device)\n",
    "        u_u_b = u_u_b.to(device)\n",
    "        t_v_b = t_v_b.to(device)\n",
    "        x_v_b = x_v_b.to(device)\n",
    "        y_v_b = y_v_b.to(device)\n",
    "        v_v_b = v_v_b.to(device)\n",
    "        t_p_b = t_p_b.to(device)\n",
    "        x_p_b = x_p_b.to(device)\n",
    "        y_p_b = y_p_b.to(device)\n",
    "        p_p_b = p_p_b.to(device)\n",
    "        t_eq_ref_b = t_eq_ref_b.to(device)\n",
    "        x_eq_ref_b = x_eq_ref_b.to(device)\n",
    "        y_eq_ref_b = y_eq_ref_b.to(device)\n",
    "        t_eq_b = t_eq_b.to(device)\n",
    "        x_eq_b = x_eq_b.to(device)\n",
    "        y_eq_b = y_eq_b.to(device)\n",
    "        \n",
    "        NS_loss = loss_NS_2D(model, t_eq_b, x_eq_b, y_eq_b)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            \n",
    "            U_loss  = loss_u(model, t_u_b, x_u_b, y_u_b, u_u_b)\n",
    "            V_loss  = loss_v(model, t_v_b, x_v_b, y_v_b, v_v_b)\n",
    "            P_loss  = loss_p(model, t_p_b, x_p_b, y_p_b, p_p_b)\n",
    "            \n",
    "    return NS_loss, U_loss, V_loss, P_loss\n",
    "            \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed4f6861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab17f351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1 | Loss training: 4.9743e-01 | NS_Loss: 1.2494e-01 | U_Loss: 4.9235e-01 | V_Loss: 4.6464e-01 | P_Loss: 3.8230e-01 | learning rate: 1.0000e-03 | time: 1.5095e+01 | \n",
      "Epoch:    2 | Loss training: 1.5368e+00 | NS_Loss: 1.9912e-01 | U_Loss: 8.0903e-01 | V_Loss: 5.9779e-01 | P_Loss: 3.3214e-01 | learning rate: 1.0000e-03 | time: 1.1257e+01 | \n",
      "Epoch:    3 | Loss training: 3.3571e+01 | NS_Loss: 3.6496e-06 | U_Loss: 1.0159e+00 | V_Loss: 4.2152e+00 | P_Loss: 5.8171e+00 | learning rate: 1.0000e-03 | time: 1.1263e+01 | \n",
      "Epoch:    4 | Loss training: 9.9059e-01 | NS_Loss: 2.7175e-04 | U_Loss: 9.8812e-01 | V_Loss: 9.6864e-01 | P_Loss: 1.0098e+00 | learning rate: 1.0000e-03 | time: 1.3660e+01 | \n",
      "Epoch:    5 | Loss training: 1.0003e+00 | NS_Loss: 9.6110e-13 | U_Loss: 9.9481e-01 | V_Loss: 1.0303e+00 | P_Loss: 1.0042e+00 | learning rate: 1.0000e-03 | time: 1.1028e+01 | \n",
      "Epoch:    6 | Loss training: 9.9659e-01 | NS_Loss: 1.4020e-12 | U_Loss: 9.9474e-01 | V_Loss: 9.9605e-01 | P_Loss: 9.9625e-01 | learning rate: 1.0000e-03 | time: 1.1040e+01 | \n",
      "Epoch:    7 | Loss training: 9.9678e-01 | NS_Loss: 5.2234e-13 | U_Loss: 1.0076e+00 | V_Loss: 1.0012e+00 | P_Loss: 9.9582e-01 | learning rate: 1.0000e-03 | time: 1.3941e+01 | \n",
      "Epoch:    8 | Loss training: 1.0046e+00 | NS_Loss: 5.1556e-12 | U_Loss: 9.9795e-01 | V_Loss: 9.9497e-01 | P_Loss: 9.9595e-01 | learning rate: 1.0000e-03 | time: 1.1044e+01 | \n",
      "Epoch:    9 | Loss training: 1.0063e+00 | NS_Loss: 4.6415e-12 | U_Loss: 9.9775e-01 | V_Loss: 9.9942e-01 | P_Loss: 9.9870e-01 | learning rate: 1.0000e-03 | time: 1.3483e+01 | \n",
      "Epoch:   10 | Loss training: 1.0030e+00 | NS_Loss: 4.2011e-14 | U_Loss: 9.9690e-01 | V_Loss: 9.9676e-01 | P_Loss: 1.0153e+00 | learning rate: 1.0000e-03 | time: 1.1345e+01 | \n",
      "Epoch:   11 | Loss training: 9.9706e-01 | NS_Loss: 3.3425e-13 | U_Loss: 1.0010e+00 | V_Loss: 9.9475e-01 | P_Loss: 9.9804e-01 | learning rate: 1.0000e-03 | time: 1.1062e+01 | \n",
      "Epoch:   12 | Loss training: 1.0049e+00 | NS_Loss: 9.6416e-14 | U_Loss: 9.9588e-01 | V_Loss: 1.0013e+00 | P_Loss: 9.9682e-01 | learning rate: 1.0000e-03 | time: 1.3691e+01 | \n",
      "Epoch:   13 | Loss training: 1.0001e+00 | NS_Loss: 1.7876e-12 | U_Loss: 9.9516e-01 | V_Loss: 9.9478e-01 | P_Loss: 1.0044e+00 | learning rate: 1.0000e-03 | time: 1.1264e+01 | \n",
      "Epoch:   14 | Loss training: 1.0043e+00 | NS_Loss: 1.7463e-13 | U_Loss: 9.9879e-01 | V_Loss: 9.9505e-01 | P_Loss: 1.0027e+00 | learning rate: 1.0000e-03 | time: 1.1021e+01 | \n",
      "Epoch:   15 | Loss training: 1.0044e+00 | NS_Loss: 6.5611e-13 | U_Loss: 9.9618e-01 | V_Loss: 9.9509e-01 | P_Loss: 9.9886e-01 | learning rate: 1.0000e-03 | time: 1.3759e+01 | \n",
      "Epoch:   16 | Loss training: 9.9812e-01 | NS_Loss: 2.3790e-13 | U_Loss: 1.0040e+00 | V_Loss: 9.9681e-01 | P_Loss: 1.0032e+00 | learning rate: 1.0000e-03 | time: 1.1178e+01 | \n",
      "Epoch:   17 | Loss training: 1.0082e+00 | NS_Loss: 7.3043e-13 | U_Loss: 1.0206e+00 | V_Loss: 9.9840e-01 | P_Loss: 1.0072e+00 | learning rate: 1.0000e-03 | time: 1.1043e+01 | \n",
      "Epoch:   18 | Loss training: 1.0007e+00 | NS_Loss: 7.8124e-13 | U_Loss: 9.9621e-01 | V_Loss: 9.9895e-01 | P_Loss: 9.9619e-01 | learning rate: 1.0000e-03 | time: 1.3739e+01 | \n",
      "Epoch:   19 | Loss training: 9.9995e-01 | NS_Loss: 7.0858e-15 | U_Loss: 1.0263e+00 | V_Loss: 9.9549e-01 | P_Loss: 9.9615e-01 | learning rate: 1.0000e-03 | time: 1.1042e+01 | \n",
      "Epoch:   20 | Loss training: 1.0029e+00 | NS_Loss: 5.4384e-13 | U_Loss: 9.9786e-01 | V_Loss: 9.9489e-01 | P_Loss: 1.0154e+00 | learning rate: 1.0000e-03 | time: 1.1039e+01 | \n",
      "Epoch:   21 | Loss training: 1.0082e+00 | NS_Loss: 8.4344e-13 | U_Loss: 9.9780e-01 | V_Loss: 1.0015e+00 | P_Loss: 9.9868e-01 | learning rate: 1.0000e-03 | time: 1.3939e+01 | \n",
      "Epoch:   22 | Loss training: 1.0071e+00 | NS_Loss: 1.9186e-12 | U_Loss: 1.0007e+00 | V_Loss: 1.0095e+00 | P_Loss: 1.0000e+00 | learning rate: 1.0000e-03 | time: 1.1037e+01 | \n",
      "Epoch:   23 | Loss training: 1.0004e+00 | NS_Loss: 1.4404e-13 | U_Loss: 1.0189e+00 | V_Loss: 9.9773e-01 | P_Loss: 9.9560e-01 | learning rate: 1.0000e-03 | time: 1.1328e+01 | \n",
      "Epoch:   24 | Loss training: 9.9694e-01 | NS_Loss: 1.0811e-13 | U_Loss: 9.9583e-01 | V_Loss: 9.9589e-01 | P_Loss: 1.0002e+00 | learning rate: 1.0000e-03 | time: 1.3495e+01 | \n",
      "Epoch:   25 | Loss training: 9.9666e-01 | NS_Loss: 2.2810e-11 | U_Loss: 9.9519e-01 | V_Loss: 1.0388e+00 | P_Loss: 1.0261e+00 | learning rate: 1.0000e-03 | time: 1.1105e+01 | \n",
      "Epoch:   26 | Loss training: 1.0045e+00 | NS_Loss: 8.5122e-12 | U_Loss: 9.9596e-01 | V_Loss: 9.9697e-01 | P_Loss: 1.0040e+00 | learning rate: 1.0000e-03 | time: 1.1378e+01 | \n",
      "Epoch:   27 | Loss training: 9.9933e-01 | NS_Loss: 1.5069e-10 | U_Loss: 1.0219e+00 | V_Loss: 1.0003e+00 | P_Loss: 9.9786e-01 | learning rate: 1.0000e-03 | time: 1.3493e+01 | \n",
      "Epoch:   28 | Loss training: 1.0031e+00 | NS_Loss: 9.9101e-12 | U_Loss: 1.0058e+00 | V_Loss: 1.0018e+00 | P_Loss: 1.0022e+00 | learning rate: 1.0000e-03 | time: 1.1198e+01 | \n",
      "Epoch:   29 | Loss training: 9.9715e-01 | NS_Loss: 1.6684e-11 | U_Loss: 9.9638e-01 | V_Loss: 1.0007e+00 | P_Loss: 1.0128e+00 | learning rate: 1.0000e-03 | time: 1.3526e+01 | \n",
      "Epoch:   30 | Loss training: 1.0154e+00 | NS_Loss: 9.3140e-11 | U_Loss: 1.0047e+00 | V_Loss: 1.0032e+00 | P_Loss: 9.9557e-01 | learning rate: 1.0000e-03 | time: 1.1447e+01 | \n",
      "Epoch:   31 | Loss training: 9.8899e-01 | NS_Loss: 8.5710e-06 | U_Loss: 9.6450e-01 | V_Loss: 9.9256e-01 | P_Loss: 1.0003e+00 | learning rate: 1.0000e-03 | time: 1.1142e+01 | \n",
      "Epoch:   32 | Loss training: 1.0009e+00 | NS_Loss: 9.1564e-15 | U_Loss: 9.9991e-01 | V_Loss: 9.9856e-01 | P_Loss: 9.9478e-01 | learning rate: 1.0000e-03 | time: 1.3458e+01 | \n",
      "Epoch:   33 | Loss training: 1.0082e+00 | NS_Loss: 4.0071e-15 | U_Loss: 9.9471e-01 | V_Loss: 9.9632e-01 | P_Loss: 9.9569e-01 | learning rate: 1.0000e-03 | time: 1.1482e+01 | \n",
      "Epoch:   34 | Loss training: 9.9768e-01 | NS_Loss: 4.3846e-15 | U_Loss: 1.0068e+00 | V_Loss: 9.9755e-01 | P_Loss: 9.9802e-01 | learning rate: 1.0000e-03 | time: 1.1138e+01 | \n",
      "Epoch:   35 | Loss training: 9.9890e-01 | NS_Loss: 7.2447e-14 | U_Loss: 9.9948e-01 | V_Loss: 1.0135e+00 | P_Loss: 1.0036e+00 | learning rate: 1.0000e-03 | time: 1.3403e+01 | \n",
      "Epoch:   36 | Loss training: 1.0049e+00 | NS_Loss: 3.6967e-15 | U_Loss: 9.9483e-01 | V_Loss: 9.9593e-01 | P_Loss: 9.9687e-01 | learning rate: 1.0000e-03 | time: 1.1427e+01 | \n",
      "Epoch:   37 | Loss training: 9.9866e-01 | NS_Loss: 6.1266e-15 | U_Loss: 9.9976e-01 | V_Loss: 1.0011e+00 | P_Loss: 1.0001e+00 | learning rate: 1.0000e-03 | time: 1.1198e+01 | \n",
      "Epoch:   38 | Loss training: 9.9675e-01 | NS_Loss: 3.5893e-14 | U_Loss: 9.9591e-01 | V_Loss: 9.9665e-01 | P_Loss: 9.9480e-01 | learning rate: 1.0000e-03 | time: 1.3446e+01 | \n",
      "Epoch:   39 | Loss training: 9.9600e-01 | NS_Loss: 4.6724e-15 | U_Loss: 1.0057e+00 | V_Loss: 1.0270e+00 | P_Loss: 1.0354e+00 | learning rate: 1.0000e-03 | time: 1.1418e+01 | \n",
      "Epoch:   40 | Loss training: 1.0005e+00 | NS_Loss: 1.0376e-15 | U_Loss: 9.9780e-01 | V_Loss: 9.9893e-01 | P_Loss: 9.9700e-01 | learning rate: 1.0000e-03 | time: 1.1262e+01 | \n",
      "Epoch:   41 | Loss training: 9.9630e-01 | NS_Loss: 1.8596e-14 | U_Loss: 9.9910e-01 | V_Loss: 9.9510e-01 | P_Loss: 1.0030e+00 | learning rate: 1.0000e-03 | time: 1.3499e+01 | \n",
      "Epoch:   42 | Loss training: 1.0028e+00 | NS_Loss: 2.8042e-15 | U_Loss: 9.9889e-01 | V_Loss: 9.9911e-01 | P_Loss: 9.9695e-01 | learning rate: 1.0000e-03 | time: 1.1324e+01 | \n",
      "Epoch:   43 | Loss training: 9.9724e-01 | NS_Loss: 2.9930e-14 | U_Loss: 1.0519e+00 | V_Loss: 9.9566e-01 | P_Loss: 1.0105e+00 | learning rate: 1.0000e-03 | time: 1.1303e+01 | \n",
      "Epoch:   44 | Loss training: 1.0072e+00 | NS_Loss: 7.4057e-15 | U_Loss: 9.9622e-01 | V_Loss: 1.0205e+00 | P_Loss: 9.9599e-01 | learning rate: 1.0000e-03 | time: 1.3485e+01 | \n",
      "Epoch:   45 | Loss training: 1.0059e+00 | NS_Loss: 7.5694e-15 | U_Loss: 9.9514e-01 | V_Loss: 1.0047e+00 | P_Loss: 9.9501e-01 | learning rate: 1.0000e-03 | time: 1.1257e+01 | \n",
      "Epoch:   46 | Loss training: 9.9701e-01 | NS_Loss: 9.0117e-15 | U_Loss: 1.0113e+00 | V_Loss: 1.0126e+00 | P_Loss: 9.9498e-01 | learning rate: 1.0000e-03 | time: 1.1329e+01 | \n",
      "Epoch:   47 | Loss training: 9.9721e-01 | NS_Loss: 1.1130e-14 | U_Loss: 9.9868e-01 | V_Loss: 9.9483e-01 | P_Loss: 9.9884e-01 | learning rate: 1.0000e-03 | time: 1.3578e+01 | \n",
      "Epoch:   48 | Loss training: 9.9502e-01 | NS_Loss: 1.6730e-16 | U_Loss: 1.0418e+00 | V_Loss: 9.9530e-01 | P_Loss: 1.0067e+00 | learning rate: 1.0000e-03 | time: 1.1270e+01 | \n",
      "Epoch:   49 | Loss training: 1.0013e+00 | NS_Loss: 1.1630e-15 | U_Loss: 9.9799e-01 | V_Loss: 9.9504e-01 | P_Loss: 9.9899e-01 | learning rate: 1.0000e-03 | time: 1.4185e+01 | \n",
      "Epoch:   50 | Loss training: 9.9638e-01 | NS_Loss: 2.8309e-16 | U_Loss: 9.9624e-01 | V_Loss: 9.9501e-01 | P_Loss: 9.9973e-01 | learning rate: 1.0000e-03 | time: 2.3056e+02 | \n",
      "Epoch:   51 | Loss training: 9.9673e-01 | NS_Loss: 7.3495e-17 | U_Loss: 9.9577e-01 | V_Loss: 1.0015e+00 | P_Loss: 9.9934e-01 | learning rate: 1.0000e-03 | time: 4.0954e+02 | \n",
      "Epoch:   52 | Loss training: 1.0110e+00 | NS_Loss: 5.3890e-16 | U_Loss: 1.0011e+00 | V_Loss: 1.0343e+00 | P_Loss: 9.9753e-01 | learning rate: 1.0000e-03 | time: 4.2903e+02 | \n",
      "Epoch:   53 | Loss training: 9.9918e-01 | NS_Loss: 1.3909e-16 | U_Loss: 1.0040e+00 | V_Loss: 9.9472e-01 | P_Loss: 1.0071e+00 | learning rate: 1.0000e-03 | time: 4.3227e+02 | \n",
      "Epoch:   54 | Loss training: 9.9813e-01 | NS_Loss: 1.7163e-16 | U_Loss: 9.9650e-01 | V_Loss: 9.9779e-01 | P_Loss: 9.9594e-01 | learning rate: 1.0000e-03 | time: 4.2990e+02 | \n",
      "Epoch:   55 | Loss training: 1.0113e+00 | NS_Loss: 1.8090e-15 | U_Loss: 9.9657e-01 | V_Loss: 9.9544e-01 | P_Loss: 9.9717e-01 | learning rate: 1.0000e-03 | time: 4.3137e+02 | \n",
      "Epoch:   56 | Loss training: 9.9660e-01 | NS_Loss: 2.4539e-16 | U_Loss: 1.0283e+00 | V_Loss: 9.9751e-01 | P_Loss: 9.9502e-01 | learning rate: 1.0000e-03 | time: 4.2912e+02 | \n",
      "Epoch:   57 | Loss training: 1.0133e+00 | NS_Loss: 2.0729e-13 | U_Loss: 1.0054e+00 | V_Loss: 1.0041e+00 | P_Loss: 1.0016e+00 | learning rate: 1.0000e-03 | time: 4.2996e+02 | \n",
      "Epoch:   58 | Loss training: 9.9882e-01 | NS_Loss: 1.2139e-15 | U_Loss: 1.0021e+00 | V_Loss: 9.9471e-01 | P_Loss: 1.0147e+00 | learning rate: 1.0000e-03 | time: 4.3276e+02 | \n",
      "Epoch:   59 | Loss training: 1.0000e+00 | NS_Loss: 4.5804e-15 | U_Loss: 1.0003e+00 | V_Loss: 9.9603e-01 | P_Loss: 9.9561e-01 | learning rate: 1.0000e-03 | time: 4.3025e+02 | \n",
      "Epoch:   60 | Loss training: 1.8488e+00 | NS_Loss: 4.9922e-15 | U_Loss: 1.1124e+00 | V_Loss: 1.8253e+00 | P_Loss: 1.0268e+00 | learning rate: 1.0000e-03 | time: 4.3090e+02 | \n"
     ]
    }
   ],
   "source": [
    "# Obten la mejor pérdida \n",
    "major_loss_validation = float('inf')\n",
    "\n",
    "# Keep results for plotting\n",
    "train_loss_results = []\n",
    "NS_loss_results = []\n",
    "P_loss_results = []\n",
    "U_loss_results = []\n",
    "V_loss_results = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    t0 = time()\n",
    "    # Inicializamos registros de las funciones de perdida\n",
    "    epoch_loss_avg = RunningMean()\n",
    "    epoch_NS_loss_avg = RunningMean()\n",
    "    epoch_P_loss_avg = RunningMean()\n",
    "    epoch_U_loss_avg = RunningMean()\n",
    "    epoch_V_loss_avg = RunningMean()\n",
    "\n",
    "    # --------- ENTRENAMIENTO ---------\n",
    "    model.train()\n",
    "    loss_train = train(loader_u, loader_v, loader_p, loader_eqns_ref, loader_eqns)\n",
    "    \n",
    "     # --------- VALIDACIÓN (SIN GRADIENTES) ---------\n",
    "    model.eval()\n",
    "    NS_loss, U_loss, V_loss, P_loss = eval(loader_u, loader_v, loader_p, loader_eqns_ref, loader_eqns)\n",
    "    \n",
    "    # End epoch\n",
    "    train_loss_results.append(loss_train)\n",
    "    NS_loss_results.append(NS_loss)\n",
    "    U_loss_results.append(U_loss)\n",
    "    V_loss_results.append(V_loss)\n",
    "    P_loss_results.append(P_loss)\n",
    "\n",
    "    # Update learning rate\n",
    "    new_lr = adjust_learning_rate(model_optimizer, loss_train)\n",
    "    \n",
    "    tf = time() - t0\n",
    "    \n",
    "    if epoch % 1 == 0:    \n",
    "        print(f\"Epoch: {epoch:4} | \"\n",
    "            f\"Loss training: {loss_train:10.4e} | \"\n",
    "            f\"NS_Loss: {NS_loss:10.4e} | \"\n",
    "            f\"U_Loss: {U_loss:10.4e} | \"\n",
    "            f\"V_Loss: {V_loss:10.4e} | \"\n",
    "            f\"P_Loss: {P_loss:10.4e} | \"\n",
    "            f\"learning rate: {new_lr:10.4e} | \"\n",
    "            f\"time: {tf:10.4e} | \"\n",
    "            )\n",
    "\n",
    "    # print(\"\\n=== GRADIENTS ===\")\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if param.grad is not None:\n",
    "    #         print(f\"{name:20s} | grad.mean={param.grad.mean():+.4e} | grad.std={param.grad.std():.4e}\")\n",
    "    #     else:\n",
    "    #         print(f\"{name:20s} | grad=None\")\n",
    "\n",
    "    # info_model(model)\n",
    "    # input()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
